SERVICE: # Relevant only when using the Qiskit runtime service
  CHANNEL: "ibm_quantum"
  INSTANCE: "ibm-q-nus/default/default"

RUNTIME_OPTIONS: # Relevant only when using the Qiskit runtime service
  optimization_level: 3
  resilience_level: null
  max_execution_time: null
  execution:
    init_qubits: True
    rep_delay: null
  resilience:
    measure_mitigation: null
    measure_noise_learning:
      num_randomizations: null
      shots_per_randomization: null
    zne_mitigation: False
    zne:
      noise_factors: null
      extrapolator: null
    pec_mitigation: False
    pec:
      max_overhead: null
      noise_gain: null
    layer_noise_learning:
      max_layers_to_learn: null
      shots_per_randomization: null
      num_randomizations: null
      layer_pair_depths: null
  environment:
    log_level: "WARNING"
    job_tags: null
  simulator:
    seed_simulator: 100
    coupling_map: null
    basis_gates: null

BACKEND: # Backend configuration (If all set to null, the user needs to specify its own backend in q_env_config.py's get_backend() function)
  REAL_BACKEND: null # True: real or False: fake Aer backend
  NAME: null # Name of the backend
  DYNAMICS: # Use a DynamicsBackend (if fields above are not null, build a DynamicsBackend.from_backend() with the specified backend)
    USE_DYNAMICS: null # Whether to use a DynamicsBackend
    PHYSICAL_QUBITS: null # Number of qubits characterizing the environment (i.e. the full quantum circuit dimension)
    SOLVER_OPTIONS: # Solver options for the DynamicsBackend
      method: null
      atol: null
      rtol: null
      hmax: null # Maximum step size, if 'auto' the solver will automatically determine the step size with backend.dt
    CALIBRATION_FILES: null

TARGET: # Target Gate configuration
  GATE: "CX"
  # STATE: "0" # Target state (if GATE is null)
  PHYSICAL_QUBITS: [ 0, 1 ]

ENV: # Environment configuration
  SAMPLING_PAULIS: 50 # Number of Pauli strings to sample
  N_SHOTS: 10
  C_FACTOR: 0.5 # Cost factor for the reward function
  N_ACTIONS: 7 # Action vector length
  BATCH_SIZE: 256 # Number of actions to evaluate per policy iteration
  ACTION_SPACE:
    LOW: [-0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1] # [ -3.14, -3.14, -3.14, -3.14, -3.14, -3.14, -3.14 ]
    HIGH: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1] # [ 3.14, 3.14, 3.14, 3.14, 3.14, 3.14, 3.14 ]
  BENCHMARK_CYCLE: 1 # Number of steps between two fidelity benchmarks
  SEED: 100
  CHECK_ON_EXP: False # Whether to perform fidelity benchmarking with tomographic experiments or just using simulation
  TRAINING_WITH_CAL: False
  CHANNEL_ESTIMATOR: False

