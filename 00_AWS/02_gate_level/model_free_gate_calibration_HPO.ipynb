{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24ddfaf2-e5a9-48ab-bf1f-e297b2d0d4ba",
   "metadata": {},
   "source": [
    "# Quantum Gate calibration using Model Free Reinforcement Learning in AWS BraketLocalBackend\n",
    "\n",
    "We extend the state preparation scheme to a gate calibration scheme by providing multiple input states to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ab3b748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit_braket_provider import BraketLocalBackend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87f43da7-a9a7-452f-a5e8-0c0145a29d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukasvoss/anaconda3/envs/aws_braket/lib/python3.10/site-packages/qiskit_dynamics/dispatch/backends/jax.py:34: UserWarning: The functionality in the perturbation module of Qiskit Dynamics requires a JAX version <= 0.4.6, due to a bug in JAX versions > 0.4.6. For versions 0.4.4, 0.4.5, and 0.4.6, using the perturbation module functionality requires setting os.environ['JAX_JIT_PJIT_API_MERGE'] = '0' before importing JAX or Dynamics.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from typing import Optional\n",
    "module_path = os.path.abspath(os.path.join('/Users/lukasvoss/Documents/Master Wirtschaftsphysik/Masterarbeit Yale-NUS CQT/Quantum_Optimal_Control'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from quantumenvironment import QuantumEnvironment\n",
    "from helper_functions import select_optimizer, generate_model\n",
    "from qconfig import QiskitConfig\n",
    "# Qiskit imports for building RL environment (circuit level)\n",
    "from qiskit.circuit import ParameterVector, QuantumCircuit\n",
    "from qiskit.extensions import CXGate, XGate\n",
    "from qiskit.opflow import Zero, One, Plus, Minus, H, I, X, CX, S, Z\n",
    "from qiskit_ibm_runtime import QiskitRuntimeService, Estimator\n",
    "\n",
    "# Tensorflow imports for building RL agent and framework\n",
    "import tensorflow as tf\n",
    "from tensorflow_probability.python.distributions import MultivariateNormalDiag\n",
    "\n",
    "# Additional imports\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1de73f28-b701-4c18-888d-1e168d67b645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ansatz function, could be at pulse level or circuit level\n",
    "def apply_parametrized_circuit(qc: QuantumCircuit):\n",
    "    \"\"\"\n",
    "    Define ansatz circuit to be played on Quantum Computer. Should be parametrized with Qiskit ParameterVector\n",
    "    :param qc: Quantum Circuit instance to add the gates on\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # qc.num_qubits\n",
    "    global n_actions\n",
    "    params = ParameterVector('theta', n_actions)\n",
    "    qc.u(2 * np.pi * params[0], 2 * np.pi * params[1], 2 * np.pi * params[2], 0)\n",
    "    qc.u(2 * np.pi * params[3], 2 * np.pi * params[4], 2 * np.pi * params[5], 1)\n",
    "    qc.rzx(2 * np.pi * params[6], 0, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2dd0f78a-3737-4590-b8cc-6c59df04d13e",
   "metadata": {},
   "source": [
    "# Defining the QuantumEnvironment\n",
    "\n",
    "Below, we set the RL environment parameters, that is how we describe our quantum system. Below, we can choose to go through the use of Qiskit Runtime, or to speed things up by using the local CPU and a state-vector simulator to get measurement outcomes based on the ansatz circuit defined above. The Environment is defined as a class object called QuantumEnvironment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1ac9e41-0143-4b75-98b3-8684e2655308",
   "metadata": {},
   "source": [
    "## Generic information characterizing the quantum system\n",
    "\n",
    "The algorithm is built upon Qiskit modules. To specify how to address our quantum system of interest, we therefore adopt the IBM approach to define a quantum backend, on which qubits are defined and can be accessed via control actions and measurements.\n",
    "\n",
    "The cell below specifies:\n",
    "- ```qubit_tgt_register```: List of qubit indices which are specifically addressed by controls , namely the ones for which we intend to calibrate a gate upon or steer them in a specific quantum state. Note that this list could include less qubits than the total number of qubits, which can be useful when one wants to take into account crosstalk effects emerging from nearest-neigbor coupling.\n",
    "- ```sampling_Paulis```: number of Pauli observables  to be sampled from the system: the algorithm relies on the ability to process measurement outcomes to estimate the expectation value of different Pauli operators. The more observables we provide for sampling, the more properties we are able to deduce with accuracy about the actual state that was created when applying our custom controls. For a single qubit, the possible Pauli operators are $\\sigma_0=I$, $\\sigma_x=X$, $\\sigma_y=Y$, $\\sigma_z=Z$. For a general multiqubit system, the Pauli observables are tensor products of those single qubit Pauli operators. The algorithm will automatically estimate which observables are the most relevant to sample based on the provided target. The probability distribution from which those observables are sampled is derived from the Direct Fidelity Estimation (equation 3, https://link.aps.org/doi/10.1103/PhysRevLett.106.230501) algorithm. \n",
    "- ```N_shots```: Indicates how many measurements shall be done for each provided circuit (that is a specific combination of an action vector and a Pauli observable to be sampled)\n",
    "- The dimension of the action vector: Indicates the number of pulse/circuit parameters that characterize our parametrized quantum circuit.\n",
    "- ```estimator_options```: Options of the Qiskit Estimator primitive. The Estimator is the Qiskit module enabling an easy computation of Pauli expectation values. One can set options to make this process more reliable (typically by doing some error mitigation techniques in post-processing). Works only with Runtime Backend at the moment\n",
    "- ```abstraction_level``` chosen to encode our quantum circuit. One can choose here to stick to the usual circuit model of quantum computing, by using the ```QuantumCircuit``` objects from Qiskit and therefore set the ```abstraction_level``` to ```\"circuit\"```. However, depending on the task at hand, one can also prefer to use a pulse description of all the operations in our circuit. This is possible by using resources of another module of Qiskit called Qiskit Dynamics. In this case, one should define the ansatz circuit above in a pulse level fashion, and the simulation done at the Hamiltonian level, and not only via statevector calculations. In this notebook we set the ```abstraction_level``` to ```\"circuit\"```. Another notebook at the pulse level is available in the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "778b1dc1-576d-400b-85af-c7e846ba77de",
   "metadata": {},
   "outputs": [],
   "source": [
    "qubit_tgt_register = [0, 1]  # Choose which qubits of the QPU you want to address \n",
    "sampling_Paulis = 100\n",
    "N_shots = 1  # Number of shots for sampling the quantum computer for each action vector\n",
    "n_actions = 7  # Choose how many control parameters in pulse/circuit parametrization\n",
    "seed = 4000\n",
    "estimator_options = {'seed_simulator': seed,'resilience_level': 0}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb6ce01f-4de3-4af1-b1ec-36b1361c6dfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "Choose below which IBM Backend to use. As we are dealing with circuit level implementation, we can look for a backend supporting Qiskit Runtime (could be a cloud simulator, or real backend) or simply set backend to None and rely on the Estimator primitive based on statevector simulation. In either case, we need access to one Estimator primitive to run the algorithm, as the feedback from the measurement outcomes is done by calculating Pauli expectation values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "653de84e-4650-4311-86e6-81a3fa1f18c6",
   "metadata": {},
   "source": [
    "## 1. Setting up a Quantum Backend"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7cb9ac30-26aa-419a-9586-8ba4ed736cf0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Real backend initialization\n",
    "\n",
    "Uncomment the cell below to declare a Qiskit Runtime backend. You need an internet connection and an IBM Id account to access this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d79bf811-87b6-4a25-9e6e-968a4e12d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Real backend initialization:\n",
    "Run this cell only if intending to use a real backend,\n",
    "where Qiskit Runtime is enabled\n",
    "\"\"\"\n",
    "backend_name = 'ibm_perth'\n",
    "\n",
    "#service = QiskitRuntimeService(channel='ibm_quantum')\n",
    "#runtime_backend = service.get_backend(backend_name)\n",
    "#estimator_options = {'resilience_level': 0}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e2c6aba-ab98-417b-a6c4-9e8ced758758",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Simulation backend initialization\n",
    "If you want to run the algorithm over a simulation, you can use Qiskit BaseEstimator, which does not need any real backend and relies on statevector simulation.\n",
    "\n",
    "Note that you could also define a custom Aer noise model and use an Aer version of the Estimator primitive. This feature will become available soon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a478bf54-94a6-4fa9-9c96-9b991efba634",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If using Qiskit native Estimator primitive\n",
    "(statevector simulation)\n",
    "\"\"\"\n",
    "no_backend = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "420aab7a-0d0a-4f67-a4b7-aeda8bc980eb",
   "metadata": {},
   "source": [
    "### Choose backend and define Qiskit config dictionary\n",
    "Below, set the Backend that you would like to run among the above defined backend.\n",
    "Then define the config gathering all the components enabling the definition of the ```QuantumEnvironment```.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6354a24-9ba2-4ea9-8e90-f803bfe90394",
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = no_backend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b30cb6e4-b1b1-4b25-8456-80a4ac731cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap all info in one dict Qiskit_setup\n",
    "Qiskit_setup = QiskitConfig(parametrized_circuit=apply_parametrized_circuit, backend=backend,\n",
    "                            estimator_options=estimator_options)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2475763f-a14f-4ee0-bd4a-c68235351fd6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Define quantum target: State preparation or Gate calibration\n",
    "\n",
    "The target of our optimal control task can be of two different types:\n",
    "1.  An arbitrary quantum state to prepare with high accuracy\n",
    "2. A Quantum Gate to be calibrated in a noise-robust manner\n",
    "\n",
    "Both targets are dictionaries that are identified with a key stating their ```target_type```, which can be either ```\"state\"``` or ```\"gate\"```.\n",
    "\n",
    "For a gate target $G$, one can add the target quantum gate with a ```\"gate\"``` argument specifying a specific instance of a Qiskit ```Gate``` object. Here, we settle for calibrating a ```CXGate()```.\n",
    "Moreover, a gate calibration requires a set of input states $\\{|s_i\\rangle \\}$ to be provided, such that the agent can try to set the actions such that the fidelity between the anticipated ideal target state (calculated as  $G|s_i\\rangle$) and the output state are simultaneously maximized. To ensure a correlation between the average reward computed from the measurement outcomes and the average gate fidelity, the provided set of input states must be tomographically complete.\n",
    "\n",
    "For a state target, one can provide, similarly to an input state, an ideal circuit to prepare it (```\"circuit\": QuantumCircuit```, or a density matrix (key ```\"dm\": DensityMatrix```).\n",
    "\n",
    "Another important key that should figure in the dictionary is the ```\"register\"``` indicating the qubits indices that should be addressed by this target, i.e. upon which qubits should the target be engineered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adb3b2e1-c8de-4ea9-9dc6-604d2ae157b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target gate: CNOT gate\n",
    "\n",
    "circuit_Plus_i = S @ H\n",
    "circuit_Minus_i = S @ H @ X\n",
    "cnot_target = {\n",
    "    \"target_type\": \"gate\",\n",
    "    \"gate\": CXGate(\"CNOT\"),\n",
    "    \"register\": qubit_tgt_register\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e053f52-95a2-4eee-8e1b-e3e732459115",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = cnot_target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b00afb10-d015-4418-ae10-882c08d73e3e",
   "metadata": {},
   "source": [
    "## 3. Declare QuantumEnvironment object\n",
    "Running the box below declares the QuantumEnvironment instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a7ac8e-51e7-416f-ba48-968a50a5c446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define quantum environment\n",
    "\n",
    "q_env = QuantumEnvironment(target=target, abstraction_level=\"circuit\",\n",
    "                           Qiskit_config=Qiskit_setup,\n",
    "                           sampling_Pauli_space=sampling_Paulis, n_shots=N_shots, c_factor=0.25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1aaa472-18e0-4a37-8954-8b976466c9b2",
   "metadata": {},
   "source": [
    "# Defining the RL agent: PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c83c50-a672-482c-95a2-47daebd7bd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "Hyperparameters for RL agent\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "%time\n",
    "\n",
    "n_epochs = 1000  # Number of epochs\n",
    "batchsize = 300  # Batch size (iterate over a bunch of actions per policy to estimate expected return)\n",
    "opti = \"Adam\"  # Optimizer choice\n",
    "eta = 0.0018  # Learning rate for policy update step\n",
    "eta_2 = None  # Learning rate for critic (value function) update step\n",
    "\n",
    "use_PPO = True\n",
    "epsilon = 0.1  # Parameter for clipping value (PPO)\n",
    "grad_clip = 0.02\n",
    "critic_loss_coeff = 0.5\n",
    "optimizer = select_optimizer(lr=eta, optimizer=opti, grad_clip=grad_clip, concurrent_optimization=True, lr2=eta_2)\n",
    "sigma_eps = 1e-3  # for numerical stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2518e6c9-80d0-4fc9-b51e-a176e8aa5a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 3)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hidden_0 (Dense)                (None, 20)           80          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "hidden_1 (Dense)                (None, 20)           420         hidden_0[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "hidden_2 (Dense)                (None, 30)           630         hidden_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mean_vec (Dense)                (None, 7)            217         hidden_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sigma_vec (Dense)               (None, 7)            217         hidden_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "critic_output (Dense)           (None, 1)            31          hidden_2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,595\n",
      "Trainable params: 1,595\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "Policy parameters\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "n_qubits = 2  # Number of qubits in the system\n",
    "N_in = n_qubits + 1  # One input for each measured qubit state (0 or 1 input for each neuron)\n",
    "hidden_units = [20, 20, 30]  # List containing number of units in each hidden layer\n",
    "\n",
    "network = generate_model((N_in,), hidden_units, n_actions, actor_critic_together=True)\n",
    "network.summary()\n",
    "init_msmt = np.zeros((1, N_in))  # Here no feedback involved, so measurement sequence is always the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3f4d6c-89c6-484e-8013-2508b711757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting tools\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "avg_return = np.zeros(n_epochs)\n",
    "fidelities = np.zeros(n_epochs)\n",
    "visualization_steps = 20\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51e905a2-cd98-4146-84fd-286ab0b710db",
   "metadata": {},
   "source": [
    "## Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cd804d-998c-43f0-8119-e28fb0e4c142",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "Training loop\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "# TODO: Use TF-Agents PPO Agent\n",
    "mu_old = tf.Variable(initial_value=network(init_msmt)[0][0], trainable=False)\n",
    "sigma_old = tf.Variable(initial_value=network(init_msmt)[1][0], trainable=False)\n",
    "\n",
    "for i in tqdm(range(n_epochs)):\n",
    "\n",
    "    Old_distrib = MultivariateNormalDiag(loc=mu_old, scale_diag=sigma_old,\n",
    "                                         validate_args=True, allow_nan_stats=False)\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "        mu, sigma, b = network(init_msmt, training=True)\n",
    "        mu = tf.squeeze(mu, axis=0)\n",
    "        sigma = tf.squeeze(sigma, axis=0)\n",
    "        b = tf.squeeze(b, axis=0)\n",
    "\n",
    "        Policy_distrib = MultivariateNormalDiag(loc=mu, scale_diag=sigma,\n",
    "                                                validate_args=True, allow_nan_stats=False)\n",
    "\n",
    "        action_vector = tf.stop_gradient(tf.clip_by_value(Policy_distrib.sample(batchsize), -1., 1.))\n",
    "\n",
    "        reward = q_env.perform_action(action_vector)\n",
    "        advantage = reward - b\n",
    "\n",
    "        if use_PPO:\n",
    "            ratio = Policy_distrib.prob(action_vector) / (tf.stop_gradient(Old_distrib.prob(action_vector)) + 1e-6)\n",
    "            actor_loss = - tf.reduce_mean(tf.minimum(advantage * ratio,\n",
    "                                                     advantage * tf.clip_by_value(ratio, 1 - epsilon, 1 + epsilon)))\n",
    "        else:  # REINFORCE algorithm\n",
    "            actor_loss = - tf.reduce_mean(advantage * Policy_distrib.log_prob(action_vector))\n",
    "\n",
    "        critic_loss = tf.reduce_mean(advantage ** 2)\n",
    "        combined_loss = actor_loss + critic_loss_coeff * critic_loss\n",
    "\n",
    "    grads = tape.gradient(combined_loss, network.trainable_variables)\n",
    "\n",
    "    # For PPO, update old parameters to have access to \"old\" policy\n",
    "    if use_PPO:\n",
    "        mu_old.assign(mu)\n",
    "        sigma_old.assign(sigma)\n",
    "\n",
    "    avg_return[i] = np.mean(q_env.reward_history, axis =1)[i]\n",
    "    fidelities[i] = q_env.avg_fidelity_history[i]\n",
    "    print(\"Gate Fidelity\", fidelities[i])\n",
    "    if i%visualization_steps == 0:\n",
    "        clear_output(wait=True) # for animation\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(np.arange(1, n_epochs, 20),avg_return[0:-1:visualization_steps], '-.', label='Average return')\n",
    "        ax.plot(np.arange(1, n_epochs, 20),fidelities[0:-1:visualization_steps], label='Average Gate Fidelity')\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"State Fidelity\")\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "        print(\"Maximum fidelity reached so far:\", np.max(fidelities), \"at Epoch\", np.argmax(fidelities))\n",
    "\n",
    "    # Apply gradients\n",
    "    optimizer.apply_gradients(zip(grads, network.trainable_variables))\n",
    "if isinstance(q_env.estimator, Estimator):\n",
    "    q_env.estimator.session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27018b75-3a90-4344-a2b8-8fba79336fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Maximum fidelity reached:\", np.max(fidelities), 'at Epoch ', np.argmax(fidelities))\n",
    "print(\"Actions yielding optimal fidelity:\", np.mean(q_env.action_history[np.argmax(fidelities)], axis=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3051b5f",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9157a4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    q_env = QuantumEnvironment(target=target, abstraction_level=\"circuit\",\n",
    "                           Qiskit_config=Qiskit_setup,\n",
    "                           sampling_Pauli_space=sampling_Paulis, n_shots=N_shots, c_factor=0.25)\n",
    "\n",
    "    # 1. Define the hyperparameter search space\n",
    "    n_epochs = trial.suggest_int('epochs', 250, 2_000)\n",
    "    batchsize = trial.suggest_int('batchsize', 50, 500)\n",
    "    opti_choice = trial.suggest_categorical('opti', ['Adam', 'SGD'])\n",
    "    eta = trial.suggest_float('eta', 1e-5, 5*1e-2, log=True)\n",
    "    eta_2 = trial.suggest_float('eta_2', 1e-4, 1e-1, log=True) if trial.suggest_int(\"use_eta_2\", 0, 1) else None\n",
    "    use_PPO = True\n",
    "    epsilon = trial.suggest_float('epsilon', 0.005, 0.5)\n",
    "    grad_clip = trial.suggest_float('grad_clip', 0.005, 1)\n",
    "    critic_loss_coeff = trial.suggest_float('critic_loss_coeff', 0.1, 2)\n",
    "    sigma_eps = trial.suggest_float('sigma_eps', 1e-5, 1e-2)\n",
    "\n",
    "    optimizer = select_optimizer(lr=eta, optimizer=opti_choice, grad_clip=grad_clip, concurrent_optimization=True, lr2=eta_2)\n",
    "    \n",
    "    mu_old = tf.Variable(initial_value=network(init_msmt)[0][0], trainable=False)\n",
    "    sigma_old = tf.Variable(initial_value=network(init_msmt)[1][0], trainable=False)\n",
    "\n",
    "    avg_return = np.zeros(n_epochs)\n",
    "    fidelities = np.zeros(n_epochs)\n",
    "\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "\n",
    "        Old_distrib = MultivariateNormalDiag(loc=mu_old, scale_diag=sigma_old,\n",
    "                                            validate_args=True, allow_nan_stats=False)\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "            mu, sigma, b = network(init_msmt, training=True)\n",
    "            mu = tf.squeeze(mu, axis=0)\n",
    "            sigma = tf.squeeze(sigma, axis=0) + sigma_eps\n",
    "            b = tf.squeeze(b, axis=0)\n",
    "\n",
    "            Policy_distrib = MultivariateNormalDiag(loc=mu, scale_diag=sigma,\n",
    "                                                    validate_args=True, allow_nan_stats=False)\n",
    "\n",
    "            action_vector = tf.stop_gradient(tf.clip_by_value(Policy_distrib.sample(batchsize), -1., 1.))\n",
    "\n",
    "            reward = q_env.perform_action(action_vector)\n",
    "            advantage = reward - b\n",
    "\n",
    "            if use_PPO:\n",
    "                ratio = Policy_distrib.prob(action_vector) / (tf.stop_gradient(Old_distrib.prob(action_vector)) + 1e-6)\n",
    "                actor_loss = - tf.reduce_mean(tf.minimum(advantage * ratio,\n",
    "                                                        advantage * tf.clip_by_value(ratio, 1 - epsilon, 1 + epsilon)))\n",
    "            else:  # REINFORCE algorithm\n",
    "                actor_loss = - tf.reduce_mean(advantage * Policy_distrib.log_prob(action_vector))\n",
    "\n",
    "            critic_loss = tf.reduce_mean(advantage ** 2)\n",
    "            combined_loss = actor_loss + critic_loss_coeff * critic_loss\n",
    "\n",
    "        grads = tape.gradient(combined_loss, network.trainable_variables)\n",
    "\n",
    "        # For PPO, update old parameters to have access to \"old\" policy\n",
    "        if use_PPO:\n",
    "            mu_old.assign(mu)\n",
    "            sigma_old.assign(sigma)\n",
    "\n",
    "        # Apply gradients\n",
    "        optimizer.apply_gradients(zip(grads, network.trainable_variables))\n",
    "\n",
    "        # print('Shape of q_env.reward_history', np.shape(q_env.reward_history))\n",
    "\n",
    "        avg_return[i] = np.mean(q_env.reward_history, axis=1)[i]\n",
    "        fidelities[i] = q_env.avg_fidelity_history[i]\n",
    "\n",
    "    # Too optimize a smooth convergence at the end of the training, I use \n",
    "    last_ten_percent = int(0.1 * n_epochs)\n",
    "    return fidelities[-last_ten_percent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "440057c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3091ce1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-01 12:41:20,019] A new study created in memory with name: no-name-e0f350db-6a3e-46dc-ae69-9bef6f9a37c6\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2023-11-01 12:50:40,445] Trial 0 finished with value: 0.3856876686113969 and parameters: {'epochs': 1598, 'batchsize': 352, 'opti': 'Adam', 'eta': 0.00019592652439099202, 'use_eta_2': 1, 'eta_2': 0.01687718920222071, 'epsilon': 0.14053390607941726, 'grad_clip': 0.668489096960573, 'critic_loss_coeff': 0.12747218891449374, 'sigma_eps': 0.004655489925743666}. Best is trial 0 with value: 0.3856876686113969.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "[I 2023-11-01 12:59:37,860] Trial 1 finished with value: 0.40093546362416643 and parameters: {'epochs': 1529, 'batchsize': 355, 'opti': 'SGD', 'eta': 0.002544090311642932, 'use_eta_2': 1, 'eta_2': 0.0990482969992133, 'epsilon': 0.370324876333389, 'grad_clip': 0.8257492818517056, 'critic_loss_coeff': 0.8331709865962353, 'sigma_eps': 0.005963197889498219}. Best is trial 1 with value: 0.40093546362416643.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "[I 2023-11-01 13:02:11,384] Trial 2 finished with value: 0.5026251902414094 and parameters: {'epochs': 785, 'batchsize': 189, 'opti': 'SGD', 'eta': 0.00047207599765907766, 'use_eta_2': 0, 'epsilon': 0.33864803020871687, 'grad_clip': 0.47724547988231025, 'critic_loss_coeff': 1.4144239842104385, 'sigma_eps': 0.008590518257807432}. Best is trial 2 with value: 0.5026251902414094.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "[I 2023-11-01 13:21:16,470] Trial 3 finished with value: 0.410998496448361 and parameters: {'epochs': 539, 'batchsize': 455, 'opti': 'SGD', 'eta': 0.001843239337517524, 'use_eta_2': 0, 'epsilon': 0.25420693203767436, 'grad_clip': 0.227479483321454, 'critic_loss_coeff': 1.1701765004484677, 'sigma_eps': 0.007795609716384293}. Best is trial 2 with value: 0.5026251902414094.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2023-11-01 13:36:30,582] Trial 4 finished with value: 0.2570031239009482 and parameters: {'epochs': 1834, 'batchsize': 101, 'opti': 'Adam', 'eta': 0.008436385831435078, 'use_eta_2': 0, 'epsilon': 0.393461224508193, 'grad_clip': 0.5177869616146537, 'critic_loss_coeff': 0.30966186601981543, 'sigma_eps': 0.005904591847858533}. Best is trial 2 with value: 0.5026251902414094.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2023-11-01 13:45:37,910] Trial 5 finished with value: 0.3471784711356808 and parameters: {'epochs': 1794, 'batchsize': 295, 'opti': 'Adam', 'eta': 1.2589205058905863e-05, 'use_eta_2': 0, 'epsilon': 0.4375040648752629, 'grad_clip': 0.5879079186269942, 'critic_loss_coeff': 1.204852367898855, 'sigma_eps': 0.005058986840272333}. Best is trial 2 with value: 0.5026251902414094.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "[I 2023-11-01 13:50:10,627] Trial 6 finished with value: 0.4455653375265756 and parameters: {'epochs': 905, 'batchsize': 297, 'opti': 'SGD', 'eta': 0.0003012970437774453, 'use_eta_2': 0, 'epsilon': 0.05521769806198333, 'grad_clip': 0.29131700499657653, 'critic_loss_coeff': 1.5373786663855928, 'sigma_eps': 0.003829309968377229}. Best is trial 2 with value: 0.5026251902414094.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "[I 2023-11-01 14:09:04,982] Trial 7 finished with value: 0.3698746428935935 and parameters: {'epochs': 1898, 'batchsize': 444, 'opti': 'SGD', 'eta': 0.0009786449866283665, 'use_eta_2': 0, 'epsilon': 0.3575929452014295, 'grad_clip': 0.8113063395590626, 'critic_loss_coeff': 0.7591848956988243, 'sigma_eps': 0.0021170448851476034}. Best is trial 2 with value: 0.5026251902414094.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "[I 2023-11-01 14:10:32,487] Trial 8 finished with value: 0.38770758671479283 and parameters: {'epochs': 428, 'batchsize': 187, 'opti': 'SGD', 'eta': 0.0007709684960546999, 'use_eta_2': 1, 'eta_2': 0.00020799608294807818, 'epsilon': 0.14090954467201958, 'grad_clip': 0.47486277420902206, 'critic_loss_coeff': 1.3380932455037353, 'sigma_eps': 0.004430783904041253}. Best is trial 2 with value: 0.5026251902414094.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2023-11-01 14:13:10,934] Trial 9 finished with value: 0.8118297236340062 and parameters: {'epochs': 976, 'batchsize': 141, 'opti': 'Adam', 'eta': 0.04469475021117739, 'use_eta_2': 1, 'eta_2': 0.0012867893257918531, 'epsilon': 0.38204036560257804, 'grad_clip': 0.1359326893416035, 'critic_loss_coeff': 1.131257491207052, 'sigma_eps': 0.007032417482467064}. Best is trial 9 with value: 0.8118297236340062.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2023-11-01 14:14:54,732] Trial 10 finished with value: 0.5252013284143058 and parameters: {'epochs': 1185, 'batchsize': 61, 'opti': 'Adam', 'eta': 0.04127295321611838, 'use_eta_2': 1, 'eta_2': 0.0007966169695404922, 'epsilon': 0.4474184231986268, 'grad_clip': 0.04744985525081655, 'critic_loss_coeff': 1.9897020209442602, 'sigma_eps': 0.009637669547725533}. Best is trial 9 with value: 0.8118297236340062.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2023-11-01 14:16:30,516] Trial 11 finished with value: 0.7591775858624612 and parameters: {'epochs': 1223, 'batchsize': 50, 'opti': 'Adam', 'eta': 0.033479882665060336, 'use_eta_2': 1, 'eta_2': 0.0007762199755303332, 'epsilon': 0.4947830308503884, 'grad_clip': 0.04844869338392914, 'critic_loss_coeff': 1.9891618025711435, 'sigma_eps': 0.009418465213966373}. Best is trial 9 with value: 0.8118297236340062.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2023-11-01 14:19:45,188] Trial 12 finished with value: 0.9668451026506234 and parameters: {'epochs': 1183, 'batchsize': 145, 'opti': 'Adam', 'eta': 0.03589730002404135, 'use_eta_2': 1, 'eta_2': 0.0017499642794076332, 'epsilon': 0.49028495674107087, 'grad_clip': 0.03900167139035235, 'critic_loss_coeff': 1.899096635343666, 'sigma_eps': 0.00994898260684245}. Best is trial 12 with value: 0.9668451026506234.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2023-11-01 14:22:51,421] Trial 13 finished with value: 0.48915145774761065 and parameters: {'epochs': 966, 'batchsize': 170, 'opti': 'Adam', 'eta': 0.049384324935100976, 'use_eta_2': 1, 'eta_2': 0.0026313012467686784, 'epsilon': 0.48050114403651273, 'grad_clip': 0.006918608736193821, 'critic_loss_coeff': 1.7235667219372377, 'sigma_eps': 0.007547071741282153}. Best is trial 12 with value: 0.9668451026506234.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2023-11-01 14:26:43,024] Trial 14 finished with value: 0.9887637062334755 and parameters: {'epochs': 1409, 'batchsize': 140, 'opti': 'Adam', 'eta': 0.009772211439226356, 'use_eta_2': 1, 'eta_2': 0.0032555716057720238, 'epsilon': 0.31319770595667695, 'grad_clip': 0.22087566641664336, 'critic_loss_coeff': 1.673656562770535, 'sigma_eps': 0.009934995233329202}. Best is trial 14 with value: 0.9887637062334755.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2023-11-01 14:32:12,458] Trial 15 finished with value: 0.9859315680783853 and parameters: {'epochs': 1375, 'batchsize': 224, 'opti': 'Adam', 'eta': 0.010656463264454574, 'use_eta_2': 1, 'eta_2': 0.007367315759573459, 'epsilon': 0.2885874818825913, 'grad_clip': 0.28639012360616706, 'critic_loss_coeff': 1.6722673991383925, 'sigma_eps': 0.009781561985236355}. Best is trial 14 with value: 0.9887637062334755.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2023-11-01 14:38:19,755] Trial 16 finished with value: 0.9905752895274795 and parameters: {'epochs': 1429, 'batchsize': 241, 'opti': 'Adam', 'eta': 0.006433311630683499, 'use_eta_2': 1, 'eta_2': 0.007597409020406363, 'epsilon': 0.27863086115744395, 'grad_clip': 0.3249002354437539, 'critic_loss_coeff': 1.6792018325001128, 'sigma_eps': 0.008607451563430308}. Best is trial 16 with value: 0.9905752895274795.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2023-11-01 14:45:08,211] Trial 17 finished with value: 0.9891878676027348 and parameters: {'epochs': 1554, 'batchsize': 248, 'opti': 'Adam', 'eta': 0.006202740059255821, 'use_eta_2': 1, 'eta_2': 0.00841116037556356, 'epsilon': 0.29245619763014796, 'grad_clip': 0.3733027165847813, 'critic_loss_coeff': 1.634286210384347, 'sigma_eps': 0.008464033201442447}. Best is trial 16 with value: 0.9905752895274795.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2023-11-01 14:52:10,665] Trial 18 finished with value: 0.9832555627439108 and parameters: {'epochs': 1635, 'batchsize': 243, 'opti': 'Adam', 'eta': 0.003969096031387663, 'use_eta_2': 1, 'eta_2': 0.010622842912423977, 'epsilon': 0.22628736015013454, 'grad_clip': 0.38749578792714084, 'critic_loss_coeff': 1.441992546490111, 'sigma_eps': 0.00831797821560131}. Best is trial 16 with value: 0.9905752895274795.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "[I 2023-11-01 15:00:31,196] Trial 19 finished with value: 0.989468245182711 and parameters: {'epochs': 1376, 'batchsize': 357, 'opti': 'Adam', 'eta': 0.0058659478892921476, 'use_eta_2': 1, 'eta_2': 0.034646904736706126, 'epsilon': 0.23275398007095438, 'grad_clip': 0.3330714182359234, 'critic_loss_coeff': 1.7299117759581144, 'sigma_eps': 0.008591664163082444}. Best is trial 16 with value: 0.9905752895274795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  20\n",
      "Best trial:\n",
      "Value:  0.9905752895274795\n",
      "Params: \n",
      "    epochs: 1429\n",
      "    batchsize: 241\n",
      "    opti: Adam\n",
      "    eta: 0.006433311630683499\n",
      "    use_eta_2: 1\n",
      "    eta_2: 0.007597409020406363\n",
      "    epsilon: 0.27863086115744395\n",
      "    grad_clip: 0.3249002354437539\n",
      "    critic_loss_coeff: 1.6792018325001128\n",
      "    sigma_eps: 0.008607451563430308\n"
     ]
    }
   ],
   "source": [
    "# Create study object and specify the direction is 'maximize'.\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Run the optimization\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Results\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"Value: \", trial.value)\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc4b561",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
