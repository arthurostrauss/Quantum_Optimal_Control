{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e54a6df",
   "metadata": {},
   "source": [
    "# Arbitrary state preparation using Model-Free Reinforcement Learning\n",
    "\n",
    "This notebook showcases an application of the formalism introduced in PhysRevX.12.011059 (https://doi.org/10.1103/PhysRevX.12.011059) on arbitrary qubit state preparation, as depicted in the Appendix D.2b.\n",
    "\n",
    "The implementation of the quantum environment is done here via Qiskit, using an Estimator primitive (https://qiskit.org/documentation/partners/qiskit_ibm_runtime/tutorials/how-to-getting-started-with-estimator.html) for the execution of parametrized quantum circuits and Pauli expectation sampling.\n",
    "\n",
    "Author of notebook: Arthur Strauss\n",
    "\n",
    "Updated on 21/02/2024"
   ]
  },
  {
   "cell_type": "code",
   "id": "dd6258cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T02:11:41.912230Z",
     "start_time": "2025-03-04T02:11:36.169587Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from rl_qoc import QuantumEnvironment\n",
    "from rl_qoc.helpers.tf_utils import select_optimizer, generate_model\n",
    "from rl_qoc.config import QEnvConfig, BackendConfig, ExecutionConfig\n",
    "from rl_qoc.environment import StateTarget\n",
    "\n",
    "# Qiskit imports for building RL environment (circuit level)\n",
    "from qiskit.circuit import ParameterVector, QuantumCircuit, QuantumRegister\n",
    "from qiskit.quantum_info import DensityMatrix\n",
    "\n",
    "# Tensorflow imports for building RL agent and framework\n",
    "import tensorflow as tf\n",
    "from tensorflow_probability.python.distributions import MultivariateNormalDiag\n",
    "from gymnasium.spaces import Box\n",
    "\n",
    "# Additional imports\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "3ed3cb45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T02:11:41.917516Z",
     "start_time": "2025-03-04T02:11:41.914922Z"
    }
   },
   "source": [
    "# Ansatz function, could be at pulse level or circuit level\n",
    "from rl_qoc.circuits import apply_parametrized_circuit"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "be814f33-5091-4450-9af9-3d1be700c18e",
   "metadata": {},
   "source": [
    "# Defining the QuantumEnvironment\n",
    "\n",
    "Below, we set the RL environment parameters, that is how we describe our quantum system. Below, we can choose to go through the use of Qiskit Runtime, or to speed things up by using the local CPU and a state-vector simulator to get measurement outcomes based on the ansatz circuit defined above. The Environment is defined as a class object called QuantumEnvironment."
   ]
  },
  {
   "cell_type": "code",
   "id": "778b1dc1-576d-400b-85af-c7e846ba77de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T02:11:41.947721Z",
     "start_time": "2025-03-04T02:11:41.945451Z"
    }
   },
   "source": [
    "qubit_tgt_register = [0, 1, 2]\n",
    "sampling_Paulis = 50\n",
    "batch_size = 200\n",
    "n_shots = 1\n",
    "n_actions = 14\n",
    "seed = 3590\n",
    "estimator_options = {\"seed_simulator\": seed, \"resilience_level\": 0}\n",
    "action_space = Box(low=-1, high=1, shape=(n_actions,), dtype=np.float32)\n",
    "observation_space = Box(low=-1, high=1, shape=(2,), dtype=np.float32)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "d5dd4a3c-cd17-46c7-8d8d-14abeac65ee3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Choose below which IBM Backend to use. As we are dealing with circuit level implementation, we can look for a backend supporting Qiskit Runtime (could be a cloud simulator, or real backend) or simply set backend to None and rely on the Estimator primitive based on statevector simulation. In either case, we need access to one Estimator primitive to run the algorithm, as the feedback from the measurement outcomes is done by calculating Pauli expectation values."
   ]
  },
  {
   "cell_type": "code",
   "id": "ebf6355a-bb38-47ca-91b9-04fb55fe4a14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T02:11:41.953103Z",
     "start_time": "2025-03-04T02:11:41.951604Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "Real backend initialization:\n",
    "Run this cell only if intending to use a real backend,\n",
    "where Qiskit Runtime is enabled\n",
    "\"\"\"\n",
    "\n",
    "backend_name = \"ibm_perth\"\n",
    "\n",
    "# service = QiskitRuntimeService(channel='ibm_quantum')\n",
    "# runtime_backend = service.get_backend(backend_name)\n",
    "# estimator_options = {'resilience_level': 0}"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "d92cacd5-a904-4b3e-9844-2a09a2a1a387",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T02:11:41.957723Z",
     "start_time": "2025-03-04T02:11:41.956534Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "If using Qiskit native Estimator primitive\n",
    "(statevector simulation)\n",
    "\"\"\"\n",
    "\n",
    "backend_config = BackendConfig(None, n_qubits=3, initial_layout=[0, 1, 2])"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "a8c231b6-0822-4379-a79d-21788a5ec5d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T02:11:41.962351Z",
     "start_time": "2025-03-04T02:11:41.961079Z"
    }
   },
   "source": [
    "# Choose the backend_config to use for the training\n",
    "backend_config = backend_config"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "87fca8f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T02:11:41.967797Z",
     "start_time": "2025-03-04T02:11:41.965751Z"
    }
   },
   "source": [
    "# Define here target state density matrix\n",
    "\n",
    "# Target state: GHZ state: (|000> + |111>)/sqrt(2)\n",
    "ket0, ket1 = np.array([[1.0], [0]]), np.array([[0.0], [1.0]])\n",
    "ket000, ket111 = np.kron(np.kron(ket0, ket0), ket0), np.kron(np.kron(ket1, ket1), ket1)\n",
    "GHZ_state = (ket000 + ket111) / np.sqrt(2)\n",
    "GHZ_dm = GHZ_state @ GHZ_state.conj().T\n",
    "target_state = StateTarget(state=DensityMatrix(GHZ_dm), physical_qubits=[0, 1, 2])"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "e55618c1-20c6-4114-ad5d-f0ed8652c54c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T02:11:41.973189Z",
     "start_time": "2025-03-04T02:11:41.972014Z"
    }
   },
   "source": [
    "# Wrap all info in one dict backend_config\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "87ddad2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T02:11:41.991084Z",
     "start_time": "2025-03-04T02:11:41.977308Z"
    }
   },
   "source": [
    "# Declare QuantumEnvironment variable\n",
    "q_env_config = QEnvConfig(\n",
    "    target=target_state,\n",
    "    backend_config=backend_config,\n",
    "    action_space=action_space,\n",
    "    execution_config=ExecutionConfig(\n",
    "        batch_size=batch_size,\n",
    "        sampling_paulis=sampling_Paulis,\n",
    "        n_shots=n_shots,\n",
    "    ),\n",
    "    seed=seed,\n",
    ")\n",
    "q_env = QuantumEnvironment(q_env_config, apply_parametrized_circuit)\n",
    "print(q_env.target)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StateTarget(DensityMatrix([[0.5+0.j, 0. +0.j, 0. +0.j, 0. +0.j, 0. +0.j, 0. +0.j,\n",
      "                0. +0.j, 0.5+0.j],\n",
      "               [0. +0.j, 0. +0.j, 0. +0.j, 0. +0.j, 0. +0.j, 0. +0.j,\n",
      "                0. +0.j, 0. +0.j],\n",
      "               [0. +0.j, 0. +0.j, 0. +0.j, 0. +0.j, 0. +0.j, 0. +0.j,\n",
      "                0. +0.j, 0. +0.j],\n",
      "               [0. +0.j, 0. +0.j, 0. +0.j, 0. +0.j, 0. +0.j, 0. +0.j,\n",
      "                0. +0.j, 0. +0.j],\n",
      "               [0. +0.j, 0. +0.j, 0. +0.j, 0. +0.j, 0. +0.j, 0. +0.j,\n",
      "                0. +0.j, 0. +0.j],\n",
      "               [0. +0.j, 0. +0.j, 0. +0.j, 0. +0.j, 0. +0.j, 0. +0.j,\n",
      "                0. +0.j, 0. +0.j],\n",
      "               [0. +0.j, 0. +0.j, 0. +0.j, 0. +0.j, 0. +0.j, 0. +0.j,\n",
      "                0. +0.j, 0. +0.j],\n",
      "               [0.5+0.j, 0. +0.j, 0. +0.j, 0. +0.j, 0. +0.j, 0. +0.j,\n",
      "                0. +0.j, 0.5+0.j]],\n",
      "              dims=(2, 2, 2)) on qubits [0, 1, 2])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "987c4bb0",
   "metadata": {},
   "source": [
    "We now define the Agent, which will be in general a Deep Neural Network.\n",
    "We start by defining the hyperparameters of the training"
   ]
  },
  {
   "cell_type": "code",
   "id": "0598de5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T02:11:42.027607Z",
     "start_time": "2025-03-04T02:11:42.023039Z"
    }
   },
   "source": [
    "# Hyperparameters for the agent\n",
    "n_epochs = 1500\n",
    "opti = \"Adam\"\n",
    "eta = 0.01\n",
    "eta_2 = None\n",
    "\n",
    "use_PPO = True\n",
    "epsilon = 0.2\n",
    "grad_clip = 0.1\n",
    "critic_loss_coeff = 0.5\n",
    "optimizer = select_optimizer(\n",
    "    lr=eta, optimizer=opti, grad_clip=grad_clip, concurrent_optimization=True, lr2=eta_2\n",
    ")\n",
    "sigma_eps = 1e-3"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "2462c0b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T02:11:42.095046Z",
     "start_time": "2025-03-04T02:11:42.037408Z"
    }
   },
   "source": [
    "# Policy parameters: generate NN that will output mean and variances of the policy\n",
    "\n",
    "N_in = observation_space.shape[\n",
    "    -1\n",
    "]\n",
    "hidden_units = [100, 100, 100]\n",
    "\n",
    "network = generate_model((N_in,), hidden_units, n_actions, actor_critic_together=True)\n",
    "network.summary()\n",
    "init_msmt = np.zeros(\n",
    "    (1, N_in)\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hidden_0 (Dense)                (None, 100)          300         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "hidden_1 (Dense)                (None, 100)          10100       hidden_0[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "hidden_2 (Dense)                (None, 100)          10100       hidden_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mean_vec (Dense)                (None, 14)           1414        hidden_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sigma_vec (Dense)               (None, 14)           1414        hidden_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "critic_output (Dense)           (None, 1)            101         hidden_2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 23,429\n",
      "Trainable params: 23,429\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "b369652e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T02:11:42.111524Z",
     "start_time": "2025-03-04T02:11:42.106038Z"
    }
   },
   "source": [
    "# Plotting tools\n",
    "plt.rcParams[\"figure.dpi\"] = 300\n",
    "plt.rcParams[\"savefig.dpi\"] = 300\n",
    "avg_return = np.zeros(n_epochs)\n",
    "fidelities = np.zeros(n_epochs)\n",
    "visualization_steps = 10\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "1c4a7b31",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "\n",
    "mu_old = tf.Variable(initial_value=network(init_msmt)[0][0], trainable=False)\n",
    "sigma_old = tf.Variable(initial_value=network(init_msmt)[1][0], trainable=False)\n",
    "\n",
    "for i in tqdm(range(n_epochs)):\n",
    "\n",
    "    Old_distrib = MultivariateNormalDiag(\n",
    "        loc=mu_old, scale_diag=sigma_old, validate_args=True, allow_nan_stats=False\n",
    "    )\n",
    "    obs, _ = q_env.reset()\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "        mu, sigma, b = network(np.array([obs]), training=True)\n",
    "        mu = tf.squeeze(mu, axis=0)\n",
    "        sigma = tf.squeeze(sigma, axis=0)\n",
    "        b = tf.squeeze(b, axis=0)\n",
    "        q_env.mean_action = np.array(mu)\n",
    "        q_env.std_action = np.array(sigma)\n",
    "        Policy_distrib = MultivariateNormalDiag(\n",
    "            loc=mu, scale_diag=sigma, validate_args=True, allow_nan_stats=False\n",
    "        )\n",
    "\n",
    "        action_vector = tf.stop_gradient(\n",
    "            tf.clip_by_value(Policy_distrib.sample(batch_size), -1.0, 1.0)\n",
    "        )\n",
    "\n",
    "        _, reward, _, _, _ = q_env.step(action_vector)\n",
    "        advantage = reward - b\n",
    "\n",
    "        if use_PPO:\n",
    "            ratio = Policy_distrib.prob(action_vector) / (\n",
    "                tf.stop_gradient(Old_distrib.prob(action_vector)) + 1e-7\n",
    "            )\n",
    "            actor_loss = -tf.reduce_mean(\n",
    "                tf.minimum(\n",
    "                    advantage * ratio,\n",
    "                    advantage * tf.clip_by_value(ratio, 1 - epsilon, 1 + epsilon),\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            actor_loss = -tf.reduce_mean(advantage * Policy_distrib.log_prob(action_vector))\n",
    "\n",
    "        critic_loss = tf.reduce_mean(advantage**2)\n",
    "        combined_loss = actor_loss + critic_loss_coeff * critic_loss\n",
    "\n",
    "    grads = tape.gradient(combined_loss, network.trainable_variables)\n",
    "\n",
    "    if use_PPO:\n",
    "        mu_old.assign(mu)\n",
    "        sigma_old.assign(sigma)\n",
    "    avg_return[i] = np.mean(q_env.reward_history[-1])\n",
    "    if q_env.do_benchmark():\n",
    "        fidelities[i] = q_env.fidelity_history[-1]\n",
    "    print(\"Fidelity\", fidelities[i])\n",
    "    if i % visualization_steps == 0:\n",
    "        clear_output(wait=True)\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(\n",
    "            np.arange(1, i, 1),\n",
    "            avg_return[0:i-1],\n",
    "            \"-.\",\n",
    "            label=\"Avg return\",\n",
    "        )\n",
    "        ax.plot(\n",
    "            np.arange(1, i, 1),\n",
    "            fidelities[0:i-1],\n",
    "            label=\"State Fidelity\",\n",
    "        )\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"State Fidelity\")\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "        print(\"Maximum fidelity reached so far:\", np.max(fidelities))\n",
    "\n",
    "    optimizer.apply_gradients(zip(grads, network.trainable_variables))\n",
    "q_env.close()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "d734017e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T02:17:31.527012Z",
     "start_time": "2025-03-04T02:17:31.524360Z"
    }
   },
   "source": [
    "print(\"Maximum fidelity reached:\", np.max(fidelities), \"at Epoch \", np.argmax(fidelities))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T02:18:01.040837Z",
     "start_time": "2025-03-04T02:18:00.885783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.plot(np.mean(q_env.reward_history, axis=1), label=\"Average return\")\n",
    "plt.plot(q_env.circuit_fidelity_history, label=\"State fidelity\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average return\")"
   ],
   "id": "797a78ea012331a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a88dc936458d9fb0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
