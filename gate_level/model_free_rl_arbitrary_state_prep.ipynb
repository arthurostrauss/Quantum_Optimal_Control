{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e54a6df",
   "metadata": {},
   "source": [
    "# Arbitrary state preparation using Model-Free Reinforcement Learning\n",
    "\n",
    "This notebook showcases an application of the formalism introduced in PhysRevX.12.011059 (https://doi.org/10.1103/PhysRevX.12.011059) on arbitrary qubit state preparation, as depicted in the Appendix D.2b.\n",
    "\n",
    "The implementation of the quantum environment is done here via Qiskit, using an Estimator primitive (https://qiskit.org/documentation/partners/qiskit_ibm_runtime/tutorials/how-to-getting-started-with-estimator.html) for the execution of parametrized quantum circuits and Pauli expectation sampling.\n",
    "\n",
    "Author of notebook: Arthur Strauss\n",
    "\n",
    "Updated on 21/02/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd6258cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T11:32:29.126968Z",
     "start_time": "2024-02-21T11:32:22.387778Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(\n",
    "    os.path.join(\n",
    "        \"/Users/arthurostrauss/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Coding_projects/Quantum_Optimal_Control\"\n",
    "    )\n",
    ")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from rl_qoc import QuantumEnvironment\n",
    "from rl_qoc.helper_functions import select_optimizer, generate_model\n",
    "from rl_qoc.qconfig import QiskitConfig\n",
    "\n",
    "# Qiskit imports for building RL environment (circuit level)\n",
    "\n",
    "from qiskit.circuit import ParameterVector, QuantumCircuit\n",
    "from qiskit.quantum_info import DensityMatrix\n",
    "from qiskit_ibm_runtime import QiskitRuntimeService, Estimator\n",
    "\n",
    "# Tensorflow imports for building RL agent and framework\n",
    "import tensorflow as tf\n",
    "from tensorflow_probability.python.distributions import MultivariateNormalDiag\n",
    "\n",
    "# Additional imports\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ed3cb45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T11:32:29.127105Z",
     "start_time": "2024-02-21T11:32:29.122837Z"
    }
   },
   "source": [
    "from qiskit import QuantumRegister\n",
    "\n",
    "\n",
    "# Ansatz function, could be at pulse level or circuit level\n",
    "\n",
    "\n",
    "def apply_parametrized_circuit(\n",
    "        qc: QuantumCircuit, params: ParameterVector, qreg: QuantumRegister\n",
    "):\n",
    "    \"\"\"\n",
    "    Define ansatz circuit to be played on Quantum Computer. Should be parametrized with Qiskit ParameterVector\n",
    "    :param qc: Quantum Circuit instance to add the gates on\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global n_actions\n",
    "    qc.u(2 * np.pi * params[0], 2 * np.pi * params[1], 2 * np.pi * params[2], qreg[0])\n",
    "    qc.u(2 * np.pi * params[3], 2 * np.pi * params[4], 2 * np.pi * params[5], qreg[1])\n",
    "    qc.rzx(2 * np.pi * params[6], qreg[0], qreg[1])\n",
    "    qc.u(2 * np.pi * params[7], 2 * np.pi * params[8], 2 * np.pi * params[9], qreg[1])\n",
    "    qc.u(\n",
    "        2 * np.pi * params[10], 2 * np.pi * params[11], 2 * np.pi * params[12], qreg[2]\n",
    "    )\n",
    "    qc.rzx(2 * np.pi * params[13], qreg[1], qreg[2])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "be814f33-5091-4450-9af9-3d1be700c18e",
   "metadata": {},
   "source": [
    "# Defining the QuantumEnvironment\n",
    "\n",
    "Below, we set the RL environment parameters, that is how we describe our quantum system. Below, we can choose to go through the use of Qiskit Runtime, or to speed things up by using the local CPU and a state-vector simulator to get measurement outcomes based on the ansatz circuit defined above. The Environment is defined as a class object called QuantumEnvironment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "778b1dc1-576d-400b-85af-c7e846ba77de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T11:32:29.128245Z",
     "start_time": "2024-02-21T11:32:29.126435Z"
    }
   },
   "source": [
    "from gymnasium.spaces import Box\n",
    "\n",
    "qubit_tgt_register = [0, 1, 2]  # Choose which qubits of the QPU you want to address\n",
    "sampling_Paulis = 50\n",
    "batchsize = 200  # Batch size (iterate over a bunch of actions per policy to estimate expected return)\n",
    "N_shots = 1  # Number of shots for sampling the quantum computer for each action vector\n",
    "n_actions = 14  # Choose how many control parameters in pulse/circuit parametrization\n",
    "time_steps = 1  # Number of time steps within an episode (1 means you do one readout and assign right away the reward)\n",
    "seed = 3590\n",
    "estimator_options = {\"seed_simulator\": seed, \"resilience_level\": 0}\n",
    "action_space = Box(low=-1, high=1, shape=(n_actions,), dtype=np.float32)\n",
    "observation_space = Box(low=-1, high=1, shape=(2,), dtype=np.float32)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d5dd4a3c-cd17-46c7-8d8d-14abeac65ee3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Choose below which IBM Backend to use. As we are dealing with circuit level implementation, we can look for a backend supporting Qiskit Runtime (could be a cloud simulator, or real backend) or simply set backend to None and rely on the Estimator primitive based on statevector simulation. In either case, we need access to one Estimator primitive to run the algorithm, as the feedback from the measurement outcomes is done by calculating Pauli expectation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebf6355a-bb38-47ca-91b9-04fb55fe4a14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T11:32:29.131402Z",
     "start_time": "2024-02-21T11:32:29.128730Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "Real backend initialization:\n",
    "Run this cell only if intending to use a real backend,\n",
    "where Qiskit Runtime is enabled\n",
    "\"\"\"\n",
    "\n",
    "backend_name = \"ibm_perth\"\n",
    "\n",
    "# service = QiskitRuntimeService(channel='ibm_quantum')\n",
    "# runtime_backend = service.get_backend(backend_name)\n",
    "# estimator_options = {'resilience_level': 0}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d92cacd5-a904-4b3e-9844-2a09a2a1a387",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T11:32:29.135063Z",
     "start_time": "2024-02-21T11:32:29.132970Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "If using Qiskit native Estimator primitive\n",
    "(statevector simulation)\n",
    "\"\"\"\n",
    "\n",
    "no_backend = None"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8c231b6-0822-4379-a79d-21788a5ec5d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T11:32:29.138299Z",
     "start_time": "2024-02-21T11:32:29.135231Z"
    }
   },
   "source": [
    "backend = no_backend"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87fca8f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T11:32:29.142056Z",
     "start_time": "2024-02-21T11:32:29.137462Z"
    }
   },
   "source": [
    "# Define here target state density matrix\n",
    "\n",
    "# Target state: GHZ state: (|000> + |111>)/sqrt(2)\n",
    "ket0, ket1 = np.array([[1.0], [0]]), np.array([[0.0], [1.0]])\n",
    "ket000, ket111 = np.kron(np.kron(ket0, ket0), ket0), np.kron(np.kron(ket1, ket1), ket1)\n",
    "GHZ_state = (ket000 + ket111) / np.sqrt(2)\n",
    "GHZ_dm = GHZ_state @ GHZ_state.conj().T\n",
    "target_state = {\"dm\": DensityMatrix(GHZ_dm)}\n",
    "# print(\"Target state Density matrix:\", target_state)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e55618c1-20c6-4114-ad5d-f0ed8652c54c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T11:32:29.153575Z",
     "start_time": "2024-02-21T11:32:29.142545Z"
    }
   },
   "source": [
    "# Wrap all info in one dict Qiskit_setup\n",
    "Qiskit_setup = QiskitConfig(\n",
    "    parametrized_circuit=apply_parametrized_circuit,\n",
    "    backend=backend,\n",
    "    estimator_options=estimator_options,\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87ddad2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T11:37:39.587829Z",
     "start_time": "2024-02-21T11:37:39.290035Z"
    }
   },
   "source": [
    "# Declare QuantumEnvironment variable\n",
    "from rl_qoc.qconfig import QEnvConfig, ExecutionConfig\n",
    "\n",
    "# Define quantum environment\n",
    "execution_config = ExecutionConfig(\n",
    "    n_shots=N_shots,\n",
    "    batch_size=batchsize,\n",
    "    n_reps=1,\n",
    "    sampling_paulis=sampling_Paulis,\n",
    "    seed=seed,\n",
    "    c_factor=1.0,\n",
    ")\n",
    "q_env_config = QEnvConfig(\n",
    "    target=target_state,\n",
    "    backend_config=Qiskit_setup,\n",
    "    action_space=action_space,\n",
    "    execution_config=execution_config,\n",
    ")\n",
    "q_env = QuantumEnvironment(training_config=q_env_config)\n",
    "print(q_env.target)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "987c4bb0",
   "metadata": {},
   "source": [
    "We now define the Agent, which will be in general a Deep Neural Network.\n",
    "We start by defining the hyperparameters of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0598de5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T11:37:40.125180Z",
     "start_time": "2024-02-21T11:37:40.119156Z"
    }
   },
   "source": [
    "# Hyperparameters for the agent\n",
    "n_epochs = 1500  # Number of epochs\n",
    "opti = \"Adam\"\n",
    "eta = 0.01  # Learning rate for policy update step\n",
    "eta_2 = None  # Learning rate for critic (value function) update step\n",
    "\n",
    "use_PPO = True\n",
    "epsilon = 0.2  # Parameter for clipping value (PPO)\n",
    "grad_clip = 0.1\n",
    "critic_loss_coeff = 0.5\n",
    "optimizer = select_optimizer(\n",
    "    lr=eta, optimizer=opti, grad_clip=grad_clip, concurrent_optimization=True, lr2=eta_2\n",
    ")\n",
    "sigma_eps = 1e-3  # for numerical stability"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2462c0b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T11:37:40.365369Z",
     "start_time": "2024-02-21T11:37:40.330760Z"
    }
   },
   "source": [
    "# Policy parameters: generate NN that will output mean and variances of the policy\n",
    "\n",
    "# Policy parameters\n",
    "N_in = observation_space.shape[\n",
    "    -1\n",
    "]  # One input for each measured qubit state (0 or 1 input for each neuron)\n",
    "hidden_units = [100, 100, 100]  # List containing number of units in each hidden layer\n",
    "\n",
    "network = generate_model((N_in,), hidden_units, n_actions, actor_critic_together=True)\n",
    "network.summary()\n",
    "init_msmt = np.zeros(\n",
    "    (1, N_in)\n",
    ")  # Here no feedback involved, so measurement sequence is always the same"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b369652e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T11:37:40.666060Z",
     "start_time": "2024-02-21T11:37:40.661988Z"
    }
   },
   "source": [
    "# Plotting tools\n",
    "plt.rcParams[\"figure.dpi\"] = 300\n",
    "plt.rcParams[\"savefig.dpi\"] = 300\n",
    "avg_return = np.zeros(n_epochs)\n",
    "fidelities = np.zeros(n_epochs)\n",
    "visualization_steps = 10\n",
    "%matplotlib inline"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4a7b31",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "# Training loop\n",
    "\n",
    "mu_old = tf.Variable(initial_value=network(init_msmt)[0][0], trainable=False)\n",
    "sigma_old = tf.Variable(initial_value=network(init_msmt)[1][0], trainable=False)\n",
    "\n",
    "policy_params_str = \"Policy params:\"\n",
    "\n",
    "for i in tqdm(range(n_epochs)):\n",
    "\n",
    "    Old_distrib = MultivariateNormalDiag(\n",
    "        loc=mu_old, scale_diag=sigma_old, validate_args=True, allow_nan_stats=False\n",
    "    )\n",
    "    obs, _ = q_env.reset()\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "        mu, sigma, b = network(np.array([obs]), training=True)\n",
    "        mu = tf.squeeze(mu, axis=0)\n",
    "        sigma = tf.squeeze(sigma, axis=0)\n",
    "        b = tf.squeeze(b, axis=0)\n",
    "\n",
    "        Policy_distrib = MultivariateNormalDiag(\n",
    "            loc=mu, scale_diag=sigma, validate_args=True, allow_nan_stats=False\n",
    "        )\n",
    "\n",
    "        action_vector = tf.stop_gradient(\n",
    "            tf.clip_by_value(Policy_distrib.sample(batchsize), -1.0, 1.0)\n",
    "        )\n",
    "\n",
    "        reward = q_env.perform_action(action_vector)\n",
    "        advantage = reward - b\n",
    "\n",
    "        if use_PPO:\n",
    "            ratio = Policy_distrib.prob(action_vector) / (\n",
    "                    tf.stop_gradient(Old_distrib.prob(action_vector)) + 1e-7\n",
    "            )\n",
    "            actor_loss = -tf.reduce_mean(\n",
    "                tf.minimum(\n",
    "                    advantage * ratio,\n",
    "                    advantage * tf.clip_by_value(ratio, 1 - epsilon, 1 + epsilon),\n",
    "                )\n",
    "            )\n",
    "        else:  # REINFORCE algorithm\n",
    "            actor_loss = -tf.reduce_mean(\n",
    "                advantage * Policy_distrib.log_prob(action_vector)\n",
    "            )\n",
    "\n",
    "        critic_loss = tf.reduce_mean(advantage ** 2)\n",
    "        combined_loss = actor_loss + critic_loss_coeff * critic_loss\n",
    "\n",
    "    grads = tape.gradient(combined_loss, network.trainable_variables)\n",
    "\n",
    "    # For PPO, update old parameters to have access to \"old\" policy\n",
    "    if use_PPO:\n",
    "        mu_old.assign(mu)\n",
    "        sigma_old.assign(sigma)\n",
    "    avg_return[i] = np.mean(q_env.reward_history, axis=1)[i]\n",
    "    fidelities[i] = q_env.state_fidelity_history[i]\n",
    "    print(\"Fidelity\", fidelities[i])\n",
    "    if i % visualization_steps == 0:\n",
    "        clear_output(wait=True)  # for animation\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(\n",
    "            np.arange(1, n_epochs, visualization_steps),\n",
    "            avg_return[0:-1:visualization_steps],\n",
    "            \"-.\",\n",
    "            label=\"Avg return\",\n",
    "        )\n",
    "        ax.plot(\n",
    "            np.arange(1, n_epochs, visualization_steps),\n",
    "            fidelities[0:-1:visualization_steps],\n",
    "            label=\"State Fidelity\",\n",
    "        )\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"State Fidelity\")\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "        print(\"Maximum fidelity reached so far:\", np.max(fidelities))\n",
    "\n",
    "    # Apply gradients\n",
    "    optimizer.apply_gradients(zip(grads, network.trainable_variables))\n",
    "if isinstance(q_env.estimator, Estimator):\n",
    "    q_env.estimator.session.close()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d734017e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-21T11:32:29.939255Z"
    }
   },
   "source": [
    "print(\n",
    "    \"Maximum fidelity reached:\", np.max(fidelities), \"at Epoch \", np.argmax(fidelities)\n",
    ")"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
