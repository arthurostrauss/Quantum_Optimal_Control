from typing import List, Optional, Union, Dict, Any, Tuple
from rl_qoc.rewards import Reward
from rl_qoc.agent import PPOConfig, TrainingConfig, TrainFunctionSettings, TotalUpdates

from rl_qoc.qua import QMConfig
from qiskit_qm_provider.parameter_table import InputType
from rl_qoc import GateTarget, StateTarget
import numpy as np
import os

def generate_sync_hook(
    target: Union[GateTarget, StateTarget],
    reward: Reward,
    param_bounds: List[Tuple[float, float]],
    seed: int,
    batch_size: int,
    n_shots: int,
    pauli_sampling: int,
    n_reps: int,
    num_updates: Union[TotalUpdates, int],
    input_type: InputType,
    backend_config: QMConfig,
    ppo_config: Dict[str, Any],
    output_dir: Optional[str] = None,
) -> str:
    # Déterminer le type de cible pour la génération du code approprié
    target_initialization_code = ""

    if isinstance(target, GateTarget):
        physical_qubits = tuple(target.physical_qubits)
        gate_name = target.gate.name if hasattr(target.gate, "name") else str(target.gate)
        target_initialization_code = f"""physical_qubits = {physical_qubits}
target_name = "{gate_name}"
target = GateTarget(gate=target_name, physical_qubits=physical_qubits)
"""
    elif isinstance(target, StateTarget):
        physical_qubits = tuple(target.physical_qubits)
        state_str = np.array2string(
            target.dm.data, precision=8, separator=", ", suppress_small=True
        )
        target_initialization_code = f"""
physical_qubits = {physical_qubits}
target_state = np.array({state_str}, dtype=complex)
target = StateTarget(state=target_state, physical_qubits=physical_qubits)
"""
    else:
        raise ValueError("Unsupported target type")

    # Normalize inputs
    try:
        total_updates_value = num_updates.total_updates
    except AttributeError:
        total_updates_value = int(num_updates)

    input_type_str = getattr(input_type, "name", str(input_type))

    sync_hook_code = f"""# AUTO-GENERATED FILE - DO NOT EDIT MANUALLY
# This file was automatically generated by main.py

import json
from rl_qoc.agent import PPOConfig, TrainingConfig, TrainFunctionSettings, TotalUpdates
from rl_qoc.qua import QMEnvironment, CustomQMPPO, QMConfig
from rl_qoc.qua.pi_pulse_reward.pi_pulse_reward import PiPulseReward
from iqcc_cloud_client.runtime import get_qm_job
from rl_qoc import (
    RescaleAndClipAction,
    GateTarget,
    StateTarget,
    ChannelReward,
    StateReward,
    CAFEReward,
    ExecutionConfig,
    QEnvConfig,
    BenchmarkConfig,
)
import numpy as np
from gymnasium.spaces import Box

job = get_qm_job()
{target_initialization_code}
reward = {reward.__class__.__name__}() 

# Action space specification
param_bounds = {param_bounds}  # Can be any number of bounds

# Environment execution parameters
seed = {seed}  # Master seed to make training reproducible
batch_size = {batch_size}  # Number of actions to evaluate per policy evaluation
n_shots = {n_shots}  # Minimum number of shots per fiducial evaluation
pauli_sampling = {pauli_sampling}  # Number of fiducials to compute for fidelity estimation (DFE only)
n_reps = {n_reps}  # Number of repetitions of the cycle circuit
num_updates = TotalUpdates({total_updates_value})
backend_config = QMConfig(
    num_updates=num_updates.total_updates,
    input_type="{input_type_str}",
    verbosity={backend_config.verbosity},
    timeout={backend_config.timeout}
)

execution_config = ExecutionConfig(
    batch_size=batch_size,
    sampling_paulis=pauli_sampling,
    n_shots=n_shots,
    n_reps=n_reps,
    seed=seed,
)


def create_action_space(param_bounds):
    param_bounds = np.array(param_bounds, dtype=np.float32)
    lower_bound, upper_bound = param_bounds.T
    return Box(low=lower_bound, high=upper_bound, shape=(len(param_bounds),), dtype=np.float32)


action_space = create_action_space(param_bounds)
q_env_config = QEnvConfig(
    target=target,
    backend_config=backend_config,
    action_space=action_space,
    execution_config=execution_config,
    reward=reward,
    benchmark_config=BenchmarkConfig(0),  # No benchmark for now)
)
q_env = QMEnvironment(q_env_config, job=job)
rescaled_env = RescaleAndClipAction(q_env, -1.0, 1.0)
ppo_config = PPOConfig.from_dict({ppo_config})

ppo_agent = CustomQMPPO(ppo_config, rescaled_env)

ppo_training = TrainingConfig(num_updates)
ppo_settings = TrainFunctionSettings(plot_real_time=True, print_debug=True)

results = ppo_agent.train(ppo_training, ppo_settings)
results['hardware_runtime'] = q_env.hardware_runtime

print(json.dumps(results))
"""

    # Écrire le contenu généré dans le fichier sync_hook.py
    target_output_dir = output_dir if output_dir is not None else os.path.dirname(__file__)
    sync_hook_path = os.path.join(target_output_dir, "sync_hook.py")
    with open(sync_hook_path, "w") as f:
        f.write(sync_hook_code)

    return sync_hook_path
