# AUTO-GENERATED DGX PROGRAM - DO NOT EDIT MANUALLY
# Generated by generate_dgx_program

import json
import importlib
import rl_qoc.environment.wrappers as _custom_wrappers
import gymnasium.wrappers as _gym_wrappers
import rl_qoc.rewards as _rewards_mod
from rl_qoc.agent import PPOConfig, TrainingConfig, TrainFunctionSettings, TotalUpdates
from rl_qoc.qua import QMEnvironment, QMConfig
from rl_qoc import (
    RescaleAndClipAction,
    GateTarget,
    StateTarget,
    ExecutionConfig,
    QEnvConfig,
    BenchmarkConfig,
)
import numpy as np
from gymnasium.spaces import Box

# Embedded specs
_WRAPPERS_SPEC = []
_REWARD_SPEC = {'name': 'ChannelReward', 'module': 'rl_qoc.rewards.channel.channel_reward', 'kwargs': {'num_eigenstates_per_pauli': 1, 'fiducials_seed': np.int64(259919411), 'input_states_seed': np.int64(259919412)}}
_PATH_TO_PY_WRAPPER = 'path_to_python_wrapper.py'


def _resolve_wrapper(name: str, module: str | None = None):
    if isinstance(module, str) and module:
        try:
            mod = importlib.import_module(module)
            if hasattr(mod, name):
                return getattr(mod, name)
        except Exception:
            pass
    if hasattr(_custom_wrappers, name):
        return getattr(_custom_wrappers, name)
    if hasattr(_gym_wrappers, name):
        return getattr(_gym_wrappers, name)
    raise ValueError("Unknown wrapper '%s' (module=%s)" % (name, module))


def _instantiate_reward_from_spec(spec):
    name = spec.get("name")
    kwargs = spec.get("kwargs")
    module = spec.get("module")
    cls = None
    if isinstance(module, str) and module:
        try:
            mod = importlib.import_module(module)
            cls = getattr(mod, name, None)
        except Exception:
            cls = None
    if cls is None and hasattr(_rewards_mod, name):
        cls = getattr(_rewards_mod, name)
    if cls is None:
        reward_dict = getattr(_rewards_mod, "reward_dict", {})
        if name in reward_dict:
            cls = reward_dict[name]
        else:
            lname = str(name).lower()
            cls = reward_dict.get(lname)
    if cls is None:
        raise ValueError("Unknown reward '%s' (module=%s)" % (name, module))
    if isinstance(kwargs, dict) and kwargs:
        return cls(**kwargs)
    return cls()


physical_qubits = (0,)
target_name = "x"
target = GateTarget(gate=target_name, physical_qubits=physical_qubits)

reward = _instantiate_reward_from_spec(_REWARD_SPEC)

# Action space
param_bounds = [(-1.9800000190734863, 2.0)]

# Execution parameters
seed = 1203
batch_size = 32
n_shots = 50
pauli_sampling = 100
n_reps = [1]
num_updates = TotalUpdates(50)

backend_config = QMConfig(
    num_updates=num_updates.total_updates,
    input_type="INPUT_STREAM",
    verbosity=2,
    timeout=60,
    path_to_python_wrapper=_PATH_TO_PY_WRAPPER,
)

execution_config = ExecutionConfig(
    batch_size=batch_size,
    sampling_paulis=pauli_sampling,
    n_shots=n_shots,
    n_reps=n_reps,
    seed=seed,
)

def create_action_space(param_bounds):
    param_bounds = np.array(param_bounds, dtype=np.float32)
    lower_bound, upper_bound = param_bounds.T
    return Box(low=lower_bound, high=upper_bound, shape=(len(param_bounds),), dtype=np.float32)


action_space = create_action_space(param_bounds)
q_env_config = QEnvConfig(
    target=target,
    backend_config=backend_config,
    action_space=action_space,
    execution_config=execution_config,
    reward=reward,
    benchmark_config=BenchmarkConfig(0),
)
q_env = QMEnvironment(q_env_config)

# Apply wrappers (outermost to innermost order as provided)
def _apply_wrappers(env, wrappers_spec):
    if not wrappers_spec:
        return RescaleAndClipAction(env, -1.0, 1.0)
    for spec in wrappers_spec[::-1]:
        name = spec.get("name")
        kwargs = spec.get("kwargs", {}) or {}
        module = spec.get("module", None)
        cls = _resolve_wrapper(name, module)
        env = cls(env, **kwargs)
    return env

env = _apply_wrappers(q_env, _WRAPPERS_SPEC)
ppo_config = PPOConfig.from_dict({'WANDB_CONFIG': {'ENABLED': False, 'PROJECT': None, 'ENTITY': None, 'API_KEY': None}, 'RUN_NAME': 'test', 'OPTIMIZER': 'adam', 'NUM_UPDATES': 1, 'N_EPOCHS': 8, 'MINIBATCH_SIZE': 16, 'LEARNING_RATE': 0.0005, 'GAMMA': 0.99, 'GAE_LAMBDA': 0.95, 'ENTROPY_COEF': 0.05, 'VALUE_LOSS_COEF': 0.5, 'GRADIENT_CLIP': 0.5, 'CLIP_VALUE_LOSS': True, 'CLIP_VALUE_COEF': 0.2, 'CLIP_RATIO': 0.2, 'INPUT_ACTIVATION_FUNCTION': 'identity', 'HIDDEN_lAYERS': [64, 64], 'HIDDEN_ACTIVATION_FUNCTIONS': ['tanh', 'tanh'], 'OUTPUT_ACTIVATION_MEAN': 'tanh', 'OUTPUT_ACTIVATION_STD': 'sigmoid', 'INCLUDE_CRITIC': True, 'NORMALIZE_ADVANTAGE': True, 'TRAINING_CONFIG': {'TOTAL_UPDATES': 50, 'TARGET_FIDELITIES': [0.999, 0.9999, 0.99999], 'LOOKBACK_WINDOW': 10, 'ANNEAL_LEARNING_RATE': False, 'STD_ACTIONS_EPS': '1e-2'}, 'TRAIN_FUNCTION_SETTINGS': {'PLOT_REAL_TIME': False, 'PRINT_DEBUG': False, 'NUM_PRINTS': 10, 'HPO_MODE': False, 'CLEAR_HISTORY': False, 'SAVE_DATA': False}})

from rl_qoc.qua import CustomQMPPO
ppo_agent = CustomQMPPO(ppo_config, env)

ppo_training = TrainingConfig(num_updates)
ppo_settings = TrainFunctionSettings(plot_real_time=True, print_debug=True)

results = ppo_agent.train(ppo_training, ppo_settings)
results['hardware_runtime'] = q_env.hardware_runtime

print(json.dumps(results))
