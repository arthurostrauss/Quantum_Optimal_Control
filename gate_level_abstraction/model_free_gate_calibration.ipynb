{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24ddfaf2-e5a9-48ab-bf1f-e297b2d0d4ba",
   "metadata": {},
   "source": [
    "# Quantum Gate calibration using Model Free Reinforcement Learning\n",
    "\n",
    "We extend the state preparation scheme to a gate calibration scheme by providing multiple input states to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "87f43da7-a9a7-452f-a5e8-0c0145a29d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('/Users/lukasvoss/Documents/Master Wirtschaftsphysik/Masterarbeit Yale-NUS CQT /Quantum_Optimal_Control'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from quantumenvironment import QuantumEnvironment\n",
    "from helper_functions import select_optimizer, generate_model\n",
    "from qconfig import QiskitConfig\n",
    "# Qiskit imports for building RL environment (circuit level)\n",
    "from qiskit.circuit import ParameterVector, QuantumCircuit\n",
    "from qiskit.extensions import CXGate, XGate\n",
    "from qiskit.opflow import Zero, One, Plus, Minus, H, I, X, CX, S, Z\n",
    "from qiskit_ibm_runtime import QiskitRuntimeService, Estimator\n",
    "\n",
    "# Tensorflow imports for building RL agent and framework\n",
    "import tensorflow as tf\n",
    "from tensorflow_probability.python.distributions import MultivariateNormalDiag\n",
    "\n",
    "# Additional imports\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1de73f28-b701-4c18-888d-1e168d67b645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ansatz function, could be at pulse level or circuit level\n",
    "def apply_parametrized_circuit(qc: QuantumCircuit):\n",
    "    \"\"\"\n",
    "    Define ansatz circuit to be played on Quantum Computer. Should be parametrized with Qiskit ParameterVector\n",
    "    :param qc: Quantum Circuit instance to add the gates on\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # qc.num_qubits\n",
    "    global n_actions\n",
    "    params = ParameterVector('theta', n_actions)\n",
    "    qc.u(2 * np.pi * params[0], 2 * np.pi * params[1], 2 * np.pi * params[2], 0)\n",
    "    qc.u(2 * np.pi * params[3], 2 * np.pi * params[4], 2 * np.pi * params[5], 1)\n",
    "    qc.rzx(2 * np.pi * params[6], 0, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2dd0f78a-3737-4590-b8cc-6c59df04d13e",
   "metadata": {},
   "source": [
    "# Defining the QuantumEnvironment\n",
    "\n",
    "Below, we set the RL environment parameters, that is how we describe our quantum system. Below, we can choose to go through the use of Qiskit Runtime, or to speed things up by using the local CPU and a state-vector simulator to get measurement outcomes based on the ansatz circuit defined above. The Environment is defined as a class object called QuantumEnvironment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1ac9e41-0143-4b75-98b3-8684e2655308",
   "metadata": {},
   "source": [
    "## Generic information characterizing the quantum system\n",
    "\n",
    "The algorithm is built upon Qiskit modules. To specify how to address our quantum system of interest, we therefore adopt the IBM approach to define a quantum backend, on which qubits are defined and can be accessed via control actions and measurements.\n",
    "\n",
    "The cell below specifies:\n",
    "- ```qubit_tgt_register```: List of qubit indices which are specifically addressed by controls , namely the ones for which we intend to calibrate a gate upon or steer them in a specific quantum state. Note that this list could include less qubits than the total number of qubits, which can be useful when one wants to take into account crosstalk effects emerging from nearest-neigbor coupling.\n",
    "- ```sampling_Paulis```: number of Pauli observables  to be sampled from the system: the algorithm relies on the ability to process measurement outcomes to estimate the expectation value of different Pauli operators. The more observables we provide for sampling, the more properties we are able to deduce with accuracy about the actual state that was created when applying our custom controls. For a single qubit, the possible Pauli operators are $\\sigma_0=I$, $\\sigma_x=X$, $\\sigma_y=Y$, $\\sigma_z=Z$. For a general multiqubit system, the Pauli observables are tensor products of those single qubit Pauli operators. The algorithm will automatically estimate which observables are the most relevant to sample based on the provided target. The probability distribution from which those observables are sampled is derived from the Direct Fidelity Estimation (equation 3, https://link.aps.org/doi/10.1103/PhysRevLett.106.230501) algorithm. \n",
    "- ```N_shots```: Indicates how many measurements shall be done for each provided circuit (that is a specific combination of an action vector and a Pauli observable to be sampled)\n",
    "- The dimension of the action vector: Indicates the number of pulse/circuit parameters that characterize our parametrized quantum circuit.\n",
    "- ```estimator_options```: Options of the Qiskit Estimator primitive. The Estimator is the Qiskit module enabling an easy computation of Pauli expectation values. One can set options to make this process more reliable (typically by doing some error mitigation techniques in post-processing). Works only with Runtime Backend at the moment\n",
    "- ```abstraction_level``` chosen to encode our quantum circuit. One can choose here to stick to the usual circuit model of quantum computing, by using the ```QuantumCircuit``` objects from Qiskit and therefore set the ```abstraction_level``` to ```\"circuit\"```. However, depending on the task at hand, one can also prefer to use a pulse description of all the operations in our circuit. This is possible by using resources of another module of Qiskit called Qiskit Dynamics. In this case, one should define the ansatz circuit above in a pulse level fashion, and the simulation done at the Hamiltonian level, and not only via statevector calculations. In this notebook we set the ```abstraction_level``` to ```\"circuit\"```. Another notebook at the pulse level is available in the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "778b1dc1-576d-400b-85af-c7e846ba77de",
   "metadata": {},
   "outputs": [],
   "source": [
    "qubit_tgt_register = [0, 1]  # Choose which qubits of the QPU you want to address \n",
    "sampling_Paulis = 100\n",
    "N_shots = 1  # Number of shots for sampling the quantum computer for each action vector\n",
    "n_actions = 7  # Choose how many control parameters in pulse/circuit parametrization\n",
    "seed = 4000\n",
    "estimator_options = {'seed_simulator': seed,'resilience_level': 0}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb6ce01f-4de3-4af1-b1ec-36b1361c6dfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "Choose below which IBM Backend to use. As we are dealing with circuit level implementation, we can look for a backend supporting Qiskit Runtime (could be a cloud simulator, or real backend) or simply set backend to None and rely on the Estimator primitive based on statevector simulation. In either case, we need access to one Estimator primitive to run the algorithm, as the feedback from the measurement outcomes is done by calculating Pauli expectation values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "653de84e-4650-4311-86e6-81a3fa1f18c6",
   "metadata": {},
   "source": [
    "## 1. Setting up a Quantum Backend"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7cb9ac30-26aa-419a-9586-8ba4ed736cf0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Real backend initialization\n",
    "\n",
    "Uncomment the cell below to declare a Qiskit Runtime backend. You need an internet connection and an IBM Id account to access this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d79bf811-87b6-4a25-9e6e-968a4e12d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Real backend initialization:\n",
    "Run this cell only if intending to use a real backend,\n",
    "where Qiskit Runtime is enabled\n",
    "\"\"\"\n",
    "backend_name = 'ibm_perth'\n",
    "\n",
    "#service = QiskitRuntimeService(channel='ibm_quantum')\n",
    "#runtime_backend = service.get_backend(backend_name)\n",
    "#estimator_options = {'resilience_level': 0}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e2c6aba-ab98-417b-a6c4-9e8ced758758",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Simulation backend initialization\n",
    "If you want to run the algorithm over a simulation, you can use Qiskit BaseEstimator, which does not need any real backend and relies on statevector simulation.\n",
    "\n",
    "Note that you could also define a custom Aer noise model and use an Aer version of the Estimator primitive. This feature will become available soon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a478bf54-94a6-4fa9-9c96-9b991efba634",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If using Qiskit native Estimator primitive\n",
    "(statevector simulation)\n",
    "\"\"\"\n",
    "no_backend = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "420aab7a-0d0a-4f67-a4b7-aeda8bc980eb",
   "metadata": {},
   "source": [
    "### Choose backend and define Qiskit config dictionary\n",
    "Below, set the Backend that you would like to run among the above defined backend.\n",
    "Then define the config gathering all the components enabling the definition of the ```QuantumEnvironment```.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b6354a24-9ba2-4ea9-8e90-f803bfe90394",
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = no_backend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b30cb6e4-b1b1-4b25-8456-80a4ac731cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap all info in one dict Qiskit_setup\n",
    "Qiskit_setup = QiskitConfig(parametrized_circuit=apply_parametrized_circuit, backend=backend,\n",
    "                            estimator_options=estimator_options)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2475763f-a14f-4ee0-bd4a-c68235351fd6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Define quantum target: State preparation or Gate calibration\n",
    "\n",
    "The target of our optimal control task can be of two different types:\n",
    "1.  An arbitrary quantum state to prepare with high accuracy\n",
    "2. A Quantum Gate to be calibrated in a noise-robust manner\n",
    "\n",
    "Both targets are dictionaries that are identified with a key stating their ```target_type```, which can be either ```\"state\"``` or ```\"gate\"```.\n",
    "\n",
    "For a gate target $G$, one can add the target quantum gate with a ```\"gate\"``` argument specifying a specific instance of a Qiskit ```Gate``` object. Here, we settle for calibrating a ```CXGate()```.\n",
    "Moreover, a gate calibration requires a set of input states $\\{|s_i\\rangle \\}$ to be provided, such that the agent can try to set the actions such that the fidelity between the anticipated ideal target state (calculated as  $G|s_i\\rangle$) and the output state are simultaneously maximized. To ensure a correlation between the average reward computed from the measurement outcomes and the average gate fidelity, the provided set of input states must be tomographically complete.\n",
    "\n",
    "For a state target, one can provide, similarly to an input state, an ideal circuit to prepare it (```\"circuit\": QuantumCircuit```, or a density matrix (key ```\"dm\": DensityMatrix```).\n",
    "\n",
    "Another important key that should figure in the dictionary is the ```\"register\"``` indicating the qubits indices that should be addressed by this target, i.e. upon which qubits should the target be engineered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "adb3b2e1-c8de-4ea9-9dc6-604d2ae157b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target gate: CNOT gate\n",
    "\n",
    "circuit_Plus_i = S @ H\n",
    "circuit_Minus_i = S @ H @ X\n",
    "cnot_target = {\n",
    "    \"target_type\": \"gate\",\n",
    "    \"gate\": CXGate(\"CNOT\"),\n",
    "    \"register\": qubit_tgt_register\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0e053f52-95a2-4eee-8e1b-e3e732459115",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = cnot_target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b00afb10-d015-4418-ae10-882c08d73e3e",
   "metadata": {},
   "source": [
    "## 3. Declare QuantumEnvironment object\n",
    "Running the box below declares the QuantumEnvironment instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "27a7ac8e-51e7-416f-ba48-968a50a5c446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define quantum environment\n",
    "\n",
    "q_env = QuantumEnvironment(target=target, abstraction_level=\"circuit\",\n",
    "                           Qiskit_config=Qiskit_setup,\n",
    "                           sampling_Pauli_space=sampling_Paulis, n_shots=N_shots, c_factor=0.25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1aaa472-18e0-4a37-8954-8b976466c9b2",
   "metadata": {},
   "source": [
    "# Defining the RL agent: PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "13c83c50-a672-482c-95a2-47daebd7bd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 0 ns, total: 1 µs\n",
      "Wall time: 8.11 µs\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "Hyperparameters for RL agent\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "%time\n",
    "\n",
    "n_epochs = 1000  # Number of epochs\n",
    "batchsize = 300  # Batch size (iterate over a bunch of actions per policy to estimate expected return)\n",
    "opti = \"Adam\"  # Optimizer choice\n",
    "eta = 0.0018  # Learning rate for policy update step\n",
    "eta_2 = None  # Learning rate for critic (value function) update step\n",
    "\n",
    "use_PPO = True\n",
    "epsilon = 0.1  # Parameter for clipping value (PPO)\n",
    "grad_clip = 0.02\n",
    "critic_loss_coeff = 0.5\n",
    "optimizer = select_optimizer(lr=eta, optimizer=opti, grad_clip=grad_clip, concurrent_optimization=True, lr2=eta_2)\n",
    "sigma_eps = 1e-3  # for numerical stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2518e6c9-80d0-4fc9-b51e-a176e8aa5a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 3)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hidden_0 (Dense)                (None, 20)           80          input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "hidden_1 (Dense)                (None, 20)           420         hidden_0[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "hidden_2 (Dense)                (None, 30)           630         hidden_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "mean_vec (Dense)                (None, 7)            217         hidden_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sigma_vec (Dense)               (None, 7)            217         hidden_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "critic_output (Dense)           (None, 1)            31          hidden_2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,595\n",
      "Trainable params: 1,595\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "Policy parameters\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "N_in = n_qubits + 1  # One input for each measured qubit state (0 or 1 input for each neuron)\n",
    "hidden_units = [20, 20, 30]  # List containing number of units in each hidden layer\n",
    "\n",
    "network = generate_model((N_in,), hidden_units, n_actions, actor_critic_together=True)\n",
    "network.summary()\n",
    "init_msmt = np.zeros((1, N_in))  # Here no feedback involved, so measurement sequence is always the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6d3f4d6c-89c6-484e-8013-2508b711757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting tools\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "avg_return = np.zeros(n_epochs)\n",
    "fidelities = np.zeros(n_epochs)\n",
    "visualization_steps = 20\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51e905a2-cd98-4146-84fd-286ab0b710db",
   "metadata": {},
   "source": [
    "## Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "14cd804d-998c-43f0-8119-e28fb0e4c142",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Scalar tensor has no `len()`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 27\u001b[0m\n\u001b[1;32m     22\u001b[0m Policy_distrib \u001b[39m=\u001b[39m MultivariateNormalDiag(loc\u001b[39m=\u001b[39mmu, scale_diag\u001b[39m=\u001b[39msigma,\n\u001b[1;32m     23\u001b[0m                                         validate_args\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_nan_stats\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m action_vector \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mstop_gradient(tf\u001b[39m.\u001b[39mclip_by_value(Policy_distrib\u001b[39m.\u001b[39msample(batchsize), \u001b[39m-\u001b[39m\u001b[39m1.\u001b[39m, \u001b[39m1.\u001b[39m))\n\u001b[0;32m---> 27\u001b[0m reward \u001b[39m=\u001b[39m q_env\u001b[39m.\u001b[39;49mperform_action(action_vector)\n\u001b[1;32m     28\u001b[0m advantage \u001b[39m=\u001b[39m reward \u001b[39m-\u001b[39m b\n\u001b[1;32m     30\u001b[0m \u001b[39mif\u001b[39;00m use_PPO:\n",
      "File \u001b[0;32m~/Documents/Master Wirtschaftsphysik/Masterarbeit Yale-NUS CQT /Quantum_Optimal_Control/quantumenvironment.py:278\u001b[0m, in \u001b[0;36mperform_action\u001b[0;34m(self, actions, do_benchmark)\u001b[0m\n\u001b[1;32m    275\u001b[0m angles, batch_size \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(actions), \u001b[39mlen\u001b[39m(np\u001b[39m.\u001b[39marray(actions))\n\u001b[1;32m    276\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_history\u001b[39m.\u001b[39mappend(angles)\n\u001b[0;32m--> 278\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgate\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    279\u001b[0m     \u001b[39m# Pick random input state from the list of possible input states (forming a tomographically complete set)\u001b[39;00m\n\u001b[1;32m    280\u001b[0m     index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget[\u001b[39m\"\u001b[39m\u001b[39minput_states\u001b[39m\u001b[39m\"\u001b[39m]))\n\u001b[1;32m    281\u001b[0m     input_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget[\u001b[39m\"\u001b[39m\u001b[39minput_states\u001b[39m\u001b[39m\"\u001b[39m][index]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/framework/ops.py:1083\u001b[0m, in \u001b[0;36m_EagerTensorBase.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns the length of the first dimension in the Tensor.\"\"\"\u001b[39;00m\n\u001b[1;32m   1082\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mndims:\n\u001b[0;32m-> 1083\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mScalar tensor has no `len()`\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1084\u001b[0m \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: Scalar tensor has no `len()`"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "Training loop\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "# TODO: Use TF-Agents PPO Agent\n",
    "mu_old = tf.Variable(initial_value=network(init_msmt)[0][0], trainable=False)\n",
    "sigma_old = tf.Variable(initial_value=network(init_msmt)[1][0], trainable=False)\n",
    "\n",
    "for i in tqdm(range(n_epochs)):\n",
    "\n",
    "    Old_distrib = MultivariateNormalDiag(loc=mu_old, scale_diag=sigma_old,\n",
    "                                         validate_args=True, allow_nan_stats=False)\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "        mu, sigma, b = network(init_msmt, training=True)\n",
    "        mu = tf.squeeze(mu, axis=0)\n",
    "        sigma = tf.squeeze(sigma, axis=0)\n",
    "        b = tf.squeeze(b, axis=0)\n",
    "\n",
    "        Policy_distrib = MultivariateNormalDiag(loc=mu, scale_diag=sigma,\n",
    "                                                validate_args=True, allow_nan_stats=False)\n",
    "\n",
    "        action_vector = tf.stop_gradient(tf.clip_by_value(Policy_distrib.sample(batchsize), -1., 1.))\n",
    "\n",
    "        reward = q_env.perform_action(action_vector)\n",
    "        advantage = reward - b\n",
    "\n",
    "        if use_PPO:\n",
    "            ratio = Policy_distrib.prob(action_vector) / (tf.stop_gradient(Old_distrib.prob(action_vector)) + 1e-6)\n",
    "            actor_loss = - tf.reduce_mean(tf.minimum(advantage * ratio,\n",
    "                                                     advantage * tf.clip_by_value(ratio, 1 - epsilon, 1 + epsilon)))\n",
    "        else:  # REINFORCE algorithm\n",
    "            actor_loss = - tf.reduce_mean(advantage * Policy_distrib.log_prob(action_vector))\n",
    "\n",
    "        critic_loss = tf.reduce_mean(advantage ** 2)\n",
    "        combined_loss = actor_loss + critic_loss_coeff * critic_loss\n",
    "\n",
    "    grads = tape.gradient(combined_loss, network.trainable_variables)\n",
    "\n",
    "    # For PPO, update old parameters to have access to \"old\" policy\n",
    "    if use_PPO:\n",
    "        mu_old.assign(mu)\n",
    "        sigma_old.assign(sigma)\n",
    "\n",
    "    avg_return[i] = np.mean(q_env.reward_history, axis =1)[i]\n",
    "    fidelities[i] = q_env.avg_fidelity_history[i]\n",
    "    print(\"Gate Fidelity\", fidelities[i])\n",
    "    if i%visualization_steps == 0:\n",
    "        clear_output(wait=True) # for animation\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(np.arange(1, n_epochs, 20),avg_return[0:-1:visualization_steps], '-.', label='Average return')\n",
    "        ax.plot(np.arange(1, n_epochs, 20),fidelities[0:-1:visualization_steps], label='Average Gate Fidelity')\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"State Fidelity\")\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "        print(\"Maximum fidelity reached so far:\", np.max(fidelities), \"at Epoch\", np.argmax(fidelities))\n",
    "\n",
    "    # Apply gradients\n",
    "    optimizer.apply_gradients(zip(grads, network.trainable_variables))\n",
    "if isinstance(q_env.estimator, Estimator):\n",
    "    q_env.estimator.session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27018b75-3a90-4344-a2b8-8fba79336fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum fidelity reached: 0.9740999286028235 at Epoch  973\n",
      "Actions yielding optimal fidelity: [-0.00619882  0.79153526  0.47917795 -0.7794944  -0.26397577  0.23931341\n",
      "  0.7727397 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Maximum fidelity reached:\", np.max(fidelities), 'at Epoch ', np.argmax(fidelities))\n",
    "print(\"Actions yielding optimal fidelity:\", np.mean(q_env.action_history[np.argmax(fidelities)], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36685a5e-0517-49c8-8ebe-56bb3999f0ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
