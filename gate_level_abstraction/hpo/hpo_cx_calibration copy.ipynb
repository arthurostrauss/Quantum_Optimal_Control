{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CX Calibration with HPO under the new code architecture / workflow (DEC 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n",
    "import time\n",
    "import yaml\n",
    "import pickle\n",
    "import optuna\n",
    "module_path = os.path.abspath(os.path.join('/Users/lukasvoss/Documents/Master Wirtschaftsphysik/Masterarbeit Yale-NUS CQT/Quantum_Optimal_Control'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from quantumenvironment import QuantumEnvironment\n",
    "from agent import Agent\n",
    "from gate_level_abstraction import gate_q_env_config\n",
    "from helper_functions import load_agent_from_yaml_file\n",
    "from ppo import make_train_ppo\n",
    "from qconfig import QEnvConfig\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.WARNING,\n",
    "    format=\"%(asctime)s INFO %(message)s\", # hardcoded INFO level\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    stream=sys.stdout,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(gate_q_env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gate_q_env_config.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gymnasium.wrappers.rescale_action import RescaleAction\n",
    "\n",
    "# min_action = 0\n",
    "# max_action = 1\n",
    "# wrapped_env = RescaleAction(q_env, min_action=min_action, max_action=max_action)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import HPO Params from YAML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_config = load_agent_from_yaml_file('agent_config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_params, network_config, hpo_config = load_agent_from_yaml_file('agent_config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_env = QuantumEnvironment(gate_q_env_config)\n",
    "q_env\n",
    "q_env.parameters.params\n",
    "q_env.circuit_truncations[0].draw('mpl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperparameterOptimizer:\n",
    "    def __init__(\n",
    "            self, \n",
    "            gate_q_env_config: QEnvConfig, \n",
    "            path_agent_config: str, \n",
    "            save_results_path: str, \n",
    "            log_progress: bool = True\n",
    "        ):\n",
    "        self.gate_q_env_config = gate_q_env_config\n",
    "        self.q_env = QuantumEnvironment(self.gate_q_env_config)\n",
    "        self.ppo_params, self.network_config, self.hpo_config = load_agent_from_yaml_file(path_agent_config)\n",
    "        self.save_results_path = save_results_path\n",
    "        self.log_progress = log_progress\n",
    "        \n",
    "    def objective(self, trial):\n",
    "        # Fetch hyperparameters from the trial object\n",
    "        hpo_config = self.hpo_config\n",
    "        network_config = self.network_config\n",
    "        ppo_params = self.ppo_params\n",
    "        agent_config = {\n",
    "        'N_UPDATES': trial.suggest_int('N_UPDATES', hpo_config['n_updates'][0], hpo_config['n_updates'][1]),\n",
    "        'N_EPOCHS': trial.suggest_int('N_EPOCHS', hpo_config['n_epochs'][0], hpo_config['n_epochs'][1]),\n",
    "        'MINIBATCH_SIZE': trial.suggest_categorical('MINIBATCH_SIZE', hpo_config['minibatch_size']),\n",
    "        'BATCHSIZE_MULTIPLIER': trial.suggest_int('BATCHSIZE_MULTIPLIER', hpo_config['batchsize_multiplier'][0], hpo_config['batchsize_multiplier'][1]),\n",
    "        'LR': trial.suggest_float('LR', hpo_config['learning_rate'][0], hpo_config['learning_rate'][1], log=True),\n",
    "        'GAMMA': trial.suggest_float('GAMMA', hpo_config['gamma'][0], hpo_config['gamma'][1]),\n",
    "        'GAE_LAMBDA': trial.suggest_float('GAE_LAMBDA', hpo_config['gae_lambda'][0], hpo_config['gae_lambda'][1]),\n",
    "        'ENT_COEF': trial.suggest_float('ENT_COEF', hpo_config['ent_coef'][0], hpo_config['ent_coef'][1]),\n",
    "        'V_COEF': trial.suggest_float('V_COEF', hpo_config['v_coef'][0], hpo_config['v_coef'][1]),\n",
    "        'GRADIENT_CLIP': trial.suggest_float('GRADIENT_CLIP', hpo_config['max_grad_norm'][0], hpo_config['max_grad_norm'][1]),\n",
    "        'CLIP_VALUE_COEF': trial.suggest_float('CLIP_VALUE_COEF', hpo_config['clip_value_coef'][0], hpo_config['clip_value_coef'][1]),\n",
    "        'CLIP_RATIO': trial.suggest_float('CLIP_RATIO', hpo_config['clip_ratio'][0], hpo_config['clip_ratio'][1]),\n",
    "        }\n",
    "\n",
    "        # Allowing for a range of possible batchsize / minibatch_size combinations by ensuring that batchsize is a multiple of minibatch_size\n",
    "        agent_config['BATCHSIZE'] = agent_config['MINIBATCH_SIZE'] * agent_config['BATCHSIZE_MULTIPLIER']\n",
    "        \n",
    "        agent_config['CLIP_VALUE_LOSS'] = hpo_config['clip_value_loss']\n",
    "\n",
    "        # Add network-specific hyperparameters that are not part of HPO scope\n",
    "\n",
    "        agent_config['OPTIMIZER'] = network_config['optimizer']\n",
    "        agent_config['N_UNITS'] = network_config['n_units']\n",
    "        agent_config['ACTIVATION'] = network_config['activation']\n",
    "        agent_config['INCLUDE_CRITIC'] = network_config['include_critic']\n",
    "        agent_config['NORMALIZE_ADVANTAGE'] = network_config['normalize_advantage']\n",
    "\n",
    "        agent_config['RUN_NAME'] = ppo_params['run_name']\n",
    "\n",
    "        self.q_env = QuantumEnvironment(self.gate_q_env_config)\n",
    "        q_env = self.q_env\n",
    "        q_env.batch_size = agent_config['BATCHSIZE'] # Overwrite the batch_size of the environment with the one from the agent_config\n",
    "\n",
    "        train_fn = make_train_ppo(agent_config, q_env)\n",
    "        training_results = train_fn(total_updates=agent_config['N_UPDATES'], print_debug=True, num_prints=50)\n",
    "\n",
    "        # Save the action vector associated with this trial's fidelity for future retrieval\n",
    "        trial.set_user_attr('action_vector', training_results['action_vector'])\n",
    "\n",
    "        # Use a relevant metric from training_results as the return value\n",
    "        last_ten_percent = int(0.1 * agent_config['N_UPDATES'])\n",
    "        \n",
    "        return training_results['avg_return'][-last_ten_percent]  # Return a metric to minimize or maximize\n",
    "\n",
    "    def save_best_configuration(self):\n",
    "        if self.best_trial is not None:\n",
    "            best_config = {\n",
    "                'parameters': self.best_trial.params,\n",
    "                'action_vector': self.best_trial.user_attrs['action_vector']\n",
    "            }\n",
    "\n",
    "            # Extract directory from the path\n",
    "            # directory = os.path.dirname(o,self.save_results_path)\n",
    "            # # Create directory if it does not exist\n",
    "            # if not os.path.exists(directory):\n",
    "            #     os.makedirs(directory)\n",
    "\n",
    "            pickle_file_name = os.path.join(self.save_results_path, f'reward_{round(self.best_trial.value, 6)}.pickle')\n",
    "            logging.info('{}'.format(pickle_file_name))\n",
    "            with open(pickle_file_name, 'wb') as handle:\n",
    "                pickle.dump(best_config, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print(f\"Best configuration saved to {pickle_file_name}\")\n",
    "        else:\n",
    "            print(\"No best trial data to save.\")\n",
    "    \n",
    "    def logging_progress(self, study, start_time):\n",
    "        logging.warning('---------------- FINISHED HPO ----------------')\n",
    "        logging.warning('HPO completed in {} seconds.'.format(round(time.time() - start_time, 2)))\n",
    "        logging.warning(\"Best trial:\")\n",
    "        logging.warning(\"-------------------------\")\n",
    "        logging.warning(\"  Value: {}\".format(study.best_trial.value))\n",
    "        logging.warning(\"  Parameters: \")\n",
    "        for key, value in study.best_trial.params.items():\n",
    "            logging.warning(\"    {}: {}\".format(key, value))\n",
    "\n",
    "        best_action_vector = study.best_trial.user_attrs['action_vector']\n",
    "        logging.warning('The best action vector is {}'.format(best_action_vector))\n",
    "    \n",
    "    def optimize_hyperparameters(self):\n",
    "        start_time = time.time()\n",
    "        num_HPO_trials = self.hpo_config['num_trials']\n",
    "        logging.warning('num_HPO_trials: {}'.format(num_HPO_trials))\n",
    "        logging.warning('---------------- STARTING HPO ----------------')\n",
    "\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(self.objective, n_trials=num_HPO_trials)\n",
    "        \n",
    "        if self.log_progress:\n",
    "            self.logging_progress(study, start_time)\n",
    "\n",
    "        self.best_trial = study.best_trial\n",
    "        self.save_best_configuration()\n",
    "    \n",
    "\n",
    "    @property\n",
    "    def num_hpo_trials(self):\n",
    "        return self.hpo_config.get('num_trials', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /Users/lukasvoss/Documents/Master Wirtschaftsphysik/Masterarbeit Yale-NUS CQT/Quantum_Optimal_Control/gate_level_abstraction/hpo\n"
     ]
    }
   ],
   "source": [
    "current_working_directory = os.getcwd()\n",
    "print(\"Current Working Directory:\", current_working_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r 75%|███████▍  | 50/67 [00:07<00:02,  6.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity History: []\n",
      "SparsePauliOp(['II', 'IZ', 'ZI', 'ZZ'],\n",
      "              coeffs=[ 0.25+0.j, -0.25+0.j,  0.25+0.j, -0.25+0.j])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 51/67 [00:08<00:02,  6.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean tensor([ 0.0358,  0.0685, -0.1408, -0.0609, -0.0307, -0.1166, -0.0023])\n",
      "Average return: 0.9708470901376642\n",
      "DFE Rewards Mean: 0.9708470901376642\n",
      "DFE Rewards standard dev 0.030232810960040574\n",
      "Returns Mean: 4.8713217\n",
      "Returns standard dev 3.1871753\n",
      "Advantages Mean: 1.1966571\n",
      "Advantages standard dev 3.1871753\n",
      "Fidelity History: []\n",
      "SparsePauliOp(['II', 'XX', 'YY', 'ZZ'],\n",
      "              coeffs=[ 0.25+0.j,  0.25+0.j,  0.25+0.j, -0.25+0.j])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 52/67 [00:08<00:02,  6.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean tensor([ 0.0399,  0.0854, -0.1474, -0.0383, -0.0293, -0.0796, -0.0339])\n",
      "Average return: 0.9608132418453466\n",
      "DFE Rewards Mean: 0.9608132418453466\n",
      "DFE Rewards standard dev 0.040541789155987135\n",
      "Returns Mean: 4.501082\n",
      "Returns standard dev 3.1407716\n",
      "Advantages Mean: 0.36516443\n",
      "Advantages standard dev 3.1407719\n",
      "Fidelity History: []\n",
      "SparsePauliOp(['II', 'IZ', 'ZI', 'ZZ'],\n",
      "              coeffs=[0.25+0.j, 0.25+0.j, 0.25+0.j, 0.25+0.j])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 53/67 [00:08<00:02,  6.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean tensor([ 0.0273,  0.0734, -0.1154, -0.0238, -0.0052, -0.0639, -0.0223])\n",
      "Average return: 0.979279196338172\n",
      "DFE Rewards Mean: 0.979279196338172\n",
      "DFE Rewards standard dev 0.027818928627310162\n",
      "Returns Mean: 5.7910767\n",
      "Returns standard dev 3.8715916\n",
      "Advantages Mean: 1.916023\n",
      "Advantages standard dev 3.8715916\n",
      "Fidelity History: []\n",
      "SparsePauliOp(['II', 'IY', 'XI', 'XY'],\n",
      "              coeffs=[0.25+0.j, 0.25+0.j, 0.25+0.j, 0.25+0.j])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 54/67 [00:08<00:02,  6.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean tensor([ 0.0304,  0.0988, -0.1304, -0.0141, -0.0085, -0.0939, -0.0223])\n",
      "Average return: 0.9552791256406263\n",
      "DFE Rewards Mean: 0.9552791256406263\n",
      "DFE Rewards standard dev 0.03514525591483814\n",
      "Returns Mean: 4.0111156\n",
      "Returns standard dev 2.6666675\n",
      "Advantages Mean: -0.76036906\n",
      "Advantages standard dev 2.6666677\n",
      "Fidelity History: []\n",
      "SparsePauliOp(['II', 'XY', 'YZ', 'ZX'],\n",
      "              coeffs=[ 0.25+0.j,  0.25+0.j,  0.25+0.j, -0.25+0.j])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 55/67 [00:08<00:01,  6.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean tensor([ 0.0307,  0.1126, -0.1482, -0.0240,  0.0147, -0.0578,  0.0129])\n",
      "Average return: 0.9725133292784167\n",
      "DFE Rewards Mean: 0.9725133292784167\n",
      "DFE Rewards standard dev 0.021175960876391002\n",
      "Returns Mean: 4.563728\n",
      "Returns standard dev 2.8883693\n",
      "Advantages Mean: 0.023745475\n",
      "Advantages standard dev 2.8883696\n",
      "Fidelity History: []\n",
      "SparsePauliOp(['II', 'IX', 'XI', 'XX'],\n",
      "              coeffs=[0.25+0.j, 0.25+0.j, 0.25+0.j, 0.25+0.j])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 56/67 [00:08<00:01,  6.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean tensor([-0.0494,  0.0960, -0.1311, -0.0364,  0.0244, -0.0025,  0.0030])\n",
      "Average return: 0.975607476399118\n",
      "DFE Rewards Mean: 0.975607476399118\n",
      "DFE Rewards standard dev 0.03368855785507707\n",
      "Returns Mean: 5.6759205\n",
      "Returns standard dev 3.8495462\n",
      "Advantages Mean: 0.96070737\n",
      "Advantages standard dev 3.8495462\n",
      "Fidelity History: []\n",
      "SparsePauliOp(['II', 'IZ', 'XI', 'XZ'],\n",
      "              coeffs=[ 0.25+0.j, -0.25+0.j,  0.25+0.j, -0.25+0.j])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 57/67 [00:09<00:01,  6.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean tensor([ 0.0303,  0.1277,  0.0268,  0.0128,  0.0321,  0.0643, -0.0943])\n",
      "Average return: 0.9734852798236404\n",
      "DFE Rewards Mean: 0.9734852798236404\n",
      "DFE Rewards standard dev 0.02600732056261439\n",
      "Returns Mean: 5.3287716\n",
      "Returns standard dev 3.8104749\n",
      "Advantages Mean: 0.5466284\n",
      "Advantages standard dev 3.810475\n",
      "Fidelity History: []\n",
      "SparsePauliOp(['II', 'IZ', 'ZI', 'ZZ'],\n",
      "              coeffs=[ 0.25+0.j, -0.25+0.j, -0.25+0.j,  0.25+0.j])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 58/67 [00:09<00:01,  6.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean tensor([ 0.0261,  0.1431, -0.0147, -0.0112,  0.0493,  0.0516, -0.0847])\n",
      "Average return: 0.9610550265916158\n",
      "DFE Rewards Mean: 0.9610550265916158\n",
      "DFE Rewards standard dev 0.03854370657479181\n",
      "Returns Mean: 4.462771\n",
      "Returns standard dev 3.0902038\n",
      "Advantages Mean: -0.46956384\n",
      "Advantages standard dev 3.0902035\n",
      "Fidelity History: []\n",
      "SparsePauliOp(['II', 'XY', 'YX', 'ZZ'],\n",
      "              coeffs=[ 0.25+0.j,  0.25+0.j, -0.25+0.j, -0.25+0.j])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 59/67 [00:09<00:01,  6.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean tensor([ 0.0228,  0.1809, -0.0020, -0.0222,  0.0186,  0.0634, -0.0711])\n",
      "Average return: 0.880691077384737\n",
      "DFE Rewards Mean: 0.880691077384737\n",
      "DFE Rewards standard dev 0.07900731416076825\n",
      "Returns Mean: 2.5747485\n",
      "Returns standard dev 1.7426993\n",
      "Advantages Mean: -2.4190195\n",
      "Advantages standard dev 1.7426994\n",
      "Fidelity History: []\n",
      "SparsePauliOp(['II', 'IX', 'XI', 'XX'],\n",
      "              coeffs=[0.25+0.j, 0.25+0.j, 0.25+0.j, 0.25+0.j])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 60/67 [00:09<00:01,  6.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean tensor([ 0.0325,  0.1604, -0.0292, -0.0407,  0.0111,  0.0577, -0.0488])\n",
      "Average return: 0.9472654283653918\n",
      "DFE Rewards Mean: 0.9472654283653918\n",
      "DFE Rewards standard dev 0.054698542720350685\n",
      "Returns Mean: 4.337758\n",
      "Returns standard dev 3.323619\n",
      "Advantages Mean: -0.21431923\n",
      "Advantages standard dev 3.323619\n",
      "Fidelity History: []\n",
      "SparsePauliOp(['II', 'IZ', 'ZI', 'ZZ'],\n",
      "              coeffs=[0.25+0.j, 0.25+0.j, 0.25+0.j, 0.25+0.j])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 61/67 [00:09<00:00,  6.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean tensor([ 0.0170,  0.1224, -0.0545, -0.0453,  0.0040,  0.0312, -0.0589])\n",
      "Average return: 0.9535456779489421\n",
      "DFE Rewards Mean: 0.9535456779489421\n",
      "DFE Rewards standard dev 0.044185548536247725\n",
      "Returns Mean: 4.5042276\n",
      "Returns standard dev 3.4539268\n",
      "Advantages Mean: 0.4885023\n",
      "Advantages standard dev 3.4539266\n",
      "Fidelity History: []\n",
      "SparsePauliOp(['II', 'IZ', 'XI', 'XZ'],\n",
      "              coeffs=[0.25+0.j, 0.25+0.j, 0.25+0.j, 0.25+0.j])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 62/67 [00:09<00:00,  6.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean tensor([ 0.0070,  0.1339, -0.0293, -0.0100,  0.0121,  0.0337, -0.0517])\n",
      "Average return: 0.9830354924851415\n",
      "DFE Rewards Mean: 0.9830354924851415\n",
      "DFE Rewards standard dev 0.021637822144674638\n",
      "Returns Mean: 5.8397827\n",
      "Returns standard dev 3.556098\n",
      "Advantages Mean: 1.4707433\n",
      "Advantages standard dev 3.556098\n",
      "Fidelity History: []\n",
      "SparsePauliOp(['II', 'IX', 'XI', 'XX'],\n",
      "              coeffs=[0.25+0.j, 0.25+0.j, 0.25+0.j, 0.25+0.j])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 63/67 [00:09<00:00,  6.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean tensor([ 0.0082,  0.1489, -0.0460, -0.0194, -0.0107,  0.0238, -0.0559])\n",
      "Average return: 0.976409519830648\n",
      "DFE Rewards Mean: 0.976409519830648\n",
      "DFE Rewards standard dev 0.033497593150147005\n",
      "Returns Mean: 5.5995097\n",
      "Returns standard dev 3.6163237\n",
      "Advantages Mean: 0.6678623\n",
      "Advantages standard dev 3.6163242\n",
      "Fidelity History: []\n",
      "SparsePauliOp(['II', 'XX', 'YY', 'ZZ'],\n",
      "              coeffs=[ 0.25+0.j,  0.25+0.j, -0.25+0.j,  0.25+0.j])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 64/67 [00:10<00:00,  6.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean tensor([-0.0293,  0.1370, -0.0378,  0.0024,  0.0143,  0.0326, -0.0225])\n",
      "Average return: 0.9496948215866089\n",
      "DFE Rewards Mean: 0.9496948215866089\n",
      "DFE Rewards standard dev 0.04962113634212455\n",
      "Returns Mean: 4.1823688\n",
      "Returns standard dev 3.0738745\n",
      "Advantages Mean: -0.9704109\n",
      "Advantages standard dev 3.0738745\n",
      "Fidelity History: []\n",
      "SparsePauliOp(['II', 'IZ', 'ZI', 'ZZ'],\n",
      "              coeffs=[ 0.25+0.j, -0.25+0.j, -0.25+0.j,  0.25+0.j])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 65/67 [00:10<00:00,  6.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean tensor([-0.0185,  0.0906, -0.0867, -0.0057,  0.0101,  0.0391, -0.0086])\n",
      "Average return: 0.9793269724417883\n",
      "DFE Rewards Mean: 0.9793269724417883\n",
      "DFE Rewards standard dev 0.02828989468555505\n",
      "Returns Mean: 6.1164618\n",
      "Returns standard dev 4.1362576\n",
      "Advantages Mean: 1.3727612\n",
      "Advantages standard dev 4.1362576\n",
      "Fidelity History: []\n",
      "SparsePauliOp(['II', 'XY', 'YX', 'ZZ'],\n",
      "              coeffs=[ 0.25+0.j,  0.25+0.j, -0.25+0.j, -0.25+0.j])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 66/67 [00:10<00:00,  6.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean tensor([-2.1119e-05,  1.0753e-01, -6.5102e-02, -1.1849e-02, -7.4037e-03,\n",
      "         3.5504e-02, -1.6783e-02])\n",
      "Average return: 0.976706157203956\n",
      "DFE Rewards Mean: 0.976706157203956\n",
      "DFE Rewards standard dev 0.027133439914832358\n",
      "Returns Mean: 5.786959\n",
      "Returns standard dev 4.010071\n",
      "Advantages Mean: 0.45375866\n",
      "Advantages standard dev 4.0100713\n",
      "Fidelity History: []\n",
      "SparsePauliOp(['II', 'IZ', 'XI', 'XZ'],\n",
      "              coeffs=[0.25+0.j, 0.25+0.j, 0.25+0.j, 0.25+0.j])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:10<00:00,  6.33it/s]\n",
      "[I 2023-12-22 13:30:35,071] Trial 0 finished with value: 0.9830354924851415 and parameters: {'N_UPDATES': 67, 'N_EPOCHS': 17, 'MINIBATCH_SIZE': 24, 'BATCHSIZE_MULTIPLIER': 9, 'LR': 0.0025641401686684593, 'GAMMA': 0.9826685030392163, 'GAE_LAMBDA': 0.9877962305036907, 'ENT_COEF': 0.00031484705636878425, 'V_COEF': 0.7135297842555248, 'GRADIENT_CLIP': 0.8557258195515357, 'CLIP_VALUE_COEF': 0.279946989279446, 'CLIP_RATIO': 0.17637343112413612}. Best is trial 0 with value: 0.9830354924851415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean tensor([-0.0016,  0.0736, -0.0735,  0.0160, -0.0100,  0.0486, -0.0016])\n",
      "Average return: 0.9611420185251704\n",
      "DFE Rewards Mean: 0.9611420185251704\n",
      "DFE Rewards standard dev 0.04745229069815516\n",
      "Returns Mean: 4.691361\n",
      "Returns standard dev 3.1894784\n",
      "Advantages Mean: -0.44823703\n",
      "Advantages standard dev 3.1894786\n",
      "Fidelity History: []\n",
      "2023-12-22 13:30:35 INFO ---------------- FINISHED HPO ----------------\n",
      "2023-12-22 13:30:35 INFO HPO completed in 10.65 seconds.\n",
      "2023-12-22 13:30:35 INFO Best trial:\n",
      "2023-12-22 13:30:35 INFO -------------------------\n",
      "2023-12-22 13:30:35 INFO   Value: 0.9830354924851415\n",
      "2023-12-22 13:30:35 INFO   Parameters: \n",
      "2023-12-22 13:30:35 INFO     N_UPDATES: 67\n",
      "2023-12-22 13:30:35 INFO     N_EPOCHS: 17\n",
      "2023-12-22 13:30:35 INFO     MINIBATCH_SIZE: 24\n",
      "2023-12-22 13:30:35 INFO     BATCHSIZE_MULTIPLIER: 9\n",
      "2023-12-22 13:30:35 INFO     LR: 0.0025641401686684593\n",
      "2023-12-22 13:30:35 INFO     GAMMA: 0.9826685030392163\n",
      "2023-12-22 13:30:35 INFO     GAE_LAMBDA: 0.9877962305036907\n",
      "2023-12-22 13:30:35 INFO     ENT_COEF: 0.00031484705636878425\n",
      "2023-12-22 13:30:35 INFO     V_COEF: 0.7135297842555248\n",
      "2023-12-22 13:30:35 INFO     GRADIENT_CLIP: 0.8557258195515357\n",
      "2023-12-22 13:30:35 INFO     CLIP_VALUE_COEF: 0.279946989279446\n",
      "2023-12-22 13:30:35 INFO     CLIP_RATIO: 0.17637343112413612\n",
      "2023-12-22 13:30:35 INFO The best action vector is [ 0.00344124  0.14719991  0.02341054  0.01056046 -0.01496192 -0.05776605\n",
      "  0.03218109]\n",
      "Best configuration saved to hpo_results/reward_0.983035.pickle\n"
     ]
    }
   ],
   "source": [
    "path_agent_config = 'agent_config.yaml'\n",
    "save_results_path = 'hpo_results'\n",
    "optimizer = HyperparameterOptimizer(gate_q_env_config, path_agent_config, save_results_path)\n",
    "optimizer.optimize_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.q_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Fetch hyperparameters from the trial object\n",
    "\n",
    "    agent_config = {\n",
    "        'N_UPDATES': trial.suggest_int('N_UPDATES', hpo_config['n_updates'][0], hpo_config['n_updates'][1]),\n",
    "        'N_EPOCHS': trial.suggest_int('N_EPOCHS', hpo_config['n_epochs'][0], hpo_config['n_epochs'][1]),\n",
    "        'MINIBATCH_SIZE': trial.suggest_categorical('MINIBATCH_SIZE', hpo_config['minibatch_size']),\n",
    "        'BATCHSIZE_MULTIPLIER': trial.suggest_int('BATCHSIZE_MULTIPLIER', hpo_config['batchsize_multiplier'][0], hpo_config['batchsize_multiplier'][1]),\n",
    "        'LR': trial.suggest_float('LR', hpo_config['learning_rate'][0], hpo_config['learning_rate'][1], log=True),\n",
    "        'GAMMA': trial.suggest_float('GAMMA', hpo_config['gamma'][0], hpo_config['gamma'][1]),\n",
    "        'GAE_LAMBDA': trial.suggest_float('GAE_LAMBDA', hpo_config['gae_lambda'][0], hpo_config['gae_lambda'][1]),\n",
    "        'ENT_COEF': trial.suggest_float('ENT_COEF', hpo_config['ent_coef'][0], hpo_config['ent_coef'][1]),\n",
    "        'V_COEF': trial.suggest_float('V_COEF', hpo_config['v_coef'][0], hpo_config['v_coef'][1]),\n",
    "        'GRADIENT_CLIP': trial.suggest_float('GRADIENT_CLIP', hpo_config['max_grad_norm'][0], hpo_config['max_grad_norm'][1]),\n",
    "        'CLIP_VALUE_COEF': trial.suggest_float('CLIP_VALUE_COEF', hpo_config['clip_value_coef'][0], hpo_config['clip_value_coef'][1]),\n",
    "        'CLIP_RATIO': trial.suggest_float('CLIP_RATIO', hpo_config['clip_ratio'][0], hpo_config['clip_ratio'][1]),\n",
    "    }\n",
    "\n",
    "    # Allowing for a range of possible batchsize / minibatch_size combinations by ensuring that batchsize is a multiple of minibatch_size\n",
    "    agent_config['BATCHSIZE'] = agent_config['MINIBATCH_SIZE'] * agent_config['BATCHSIZE_MULTIPLIER']\n",
    "    \n",
    "    agent_config['CLIP_VALUE_LOSS'] = hpo_config['clip_value_loss']\n",
    "\n",
    "    # Add network-specific hyperparameters that are not part of HPO scope\n",
    "    agent_config['OPTIMIZER'] = network_config['optimizer']\n",
    "    agent_config['N_UNITS'] = network_config['n_units']\n",
    "    agent_config['ACTIVATION'] = network_config['activation']\n",
    "    agent_config['INCLUDE_CRITIC'] = network_config['include_critic']\n",
    "    agent_config['NORMALIZE_ADVANTAGE'] = network_config['normalize_advantage']\n",
    "\n",
    "    agent_config['RUN_NAME'] = ppo_params['run_name']\n",
    "\n",
    "\n",
    "    q_env = QuantumEnvironment(gate_q_env_config)  # Initialize your environment\n",
    "    q_env.batch_size = agent_config['BATCHSIZE'] # Overwrite the batch_size of the environment with the one from the agent_config\n",
    "\n",
    "    train_fn = make_train_ppo(agent_config, q_env)\n",
    "    training_results = train_fn(total_updates=agent_config['N_UPDATES'], print_debug=True, num_prints=50)\n",
    "\n",
    "    # Save the action vector associated with this trial's fidelity for future retrieval\n",
    "    trial.set_user_attr('action_vector', training_results['action_vector'])\n",
    "\n",
    "    # Use a relevant metric from training_results as the return value\n",
    "    last_ten_percent = int(0.1 * agent_config['N_UPDATES'])\n",
    "    \n",
    "    return training_results['avg_return'][-last_ten_percent]  # Return a metric to minimize or maximize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "num_HPO_trials = hpo_config['num_trials']\n",
    "print('num_HPO_trials: ', num_HPO_trials)\n",
    "\n",
    "def optimize_hyperparameters():\n",
    "    study = optuna.create_study(direction=\"maximize\")  # or 'minimize' depending on your metric\n",
    "    study.optimize(objective, n_trials=num_HPO_trials)  # Number of hyperparameter configurations to try\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    best_trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", best_trial.value)\n",
    "    print(\"  Parameters: \")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "\n",
    "    best_action_vector = best_trial.user_attrs['action_vector']\n",
    "    print(f'The best action vector is: {best_action_vector}')\n",
    "\n",
    "optimize_hyperparameters()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
