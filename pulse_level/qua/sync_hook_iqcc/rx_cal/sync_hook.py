# AUTO-GENERATED FILE - DO NOT EDIT MANUALLY
# This file was automatically generated by generate_sync_hook

import json
import importlib
import rl_qoc.environment.wrappers as _custom_wrappers
import gymnasium.wrappers as _gym_wrappers
import rl_qoc.rewards as _rewards_mod
from rl_qoc.agent import PPOConfig, TrainingConfig, TrainFunctionSettings, TotalUpdates
from rl_qoc.qua import QMEnvironment, CustomQMPPO, QMConfig
from iqcc_cloud_client.runtime import get_qm_job
from rl_qoc import (
    RescaleAndClipAction,
    GateTarget,
    StateTarget,
    ExecutionConfig,
    QEnvConfig,
    BenchmarkConfig,
    ContextSamplingWrapper,
    ContextSamplingWrapperConfig,
)
import numpy as np
from gymnasium.spaces import Box

# Embedded construction specs
_WRAPPERS_SPEC = [{'name': 'RescaleAndClipAction', 'module': 'rl_qoc.environment.wrappers.custom_wrappers', 'kwargs': {'min_action': -1.0, 'max_action': 1.0}}, {'name': 'RescaleObservation', 'module': 'gymnasium.wrappers.transform_observation', 'kwargs': {'min_obs': -1.0, 'max_obs': 1.0}}, {'name': 'FlattenObservation', 'module': 'gymnasium.wrappers.transform_observation'}, {'name': 'ParametricGateContextWrapper', 'module': 'rl_qoc.environment.wrappers.parametric_gate_context_wrapper', 'kwargs': {'config': {}}}]
_REWARD_SPEC = {'name': 'ChannelReward', 'module': 'rl_qoc.rewards.channel.channel_reward', 'kwargs': {'num_eigenstates_per_pauli': 1, 'fiducials_seed': np.int64(259919411), 'input_states_seed': np.int64(259919412)}}


def _resolve_wrapper(name: str, module: str | None = None):
    '''Resolve a wrapper class by name.
    Order: explicit module > rl_qoc.environment.wrappers > gymnasium.wrappers.'''
    if isinstance(module, str) and module:
        try:
            mod = importlib.import_module(module)
            if hasattr(mod, name):
                return getattr(mod, name)
        except Exception:
            pass
    if hasattr(_custom_wrappers, name):
        return getattr(_custom_wrappers, name)
    if hasattr(_gym_wrappers, name):
        return getattr(_gym_wrappers, name)
    raise ValueError("Unknown wrapper '%s' (module=%s)" % (name, module))


def _instantiate_reward_from_spec(spec):
    # Resolve class from explicit module > rl_qoc.rewards > reward_dict
    name = spec.get("name")
    kwargs = spec.get("kwargs")
    module = spec.get("module")
    cls = None
    if isinstance(module, str) and module:
        try:
            mod = importlib.import_module(module)
            cls = getattr(mod, name, None)
        except Exception:
            cls = None
    if cls is None and hasattr(_rewards_mod, name):
        cls = getattr(_rewards_mod, name)
    if cls is None:
        reward_dict = getattr(_rewards_mod, "reward_dict", {})
        if name in reward_dict:
            cls = reward_dict[name]
        else:
            lname = str(name).lower()
            cls = reward_dict.get(lname)
    if cls is None:
        raise ValueError("Unknown reward '%s' (module=%s)" % (name, module))
    # Only pass kwargs if present and non-empty; some rewards have no-arg ctors
    if isinstance(kwargs, dict) and kwargs:
        return cls(**kwargs)
    return cls()


def _apply_wrappers(env, wrappers_spec):
    '''Apply an ordered list of wrappers to env.

    For wrappers subclassing ContextSamplingWrapper, a 'config' dict is accepted
    and converted to ContextSamplingWrapperConfig via from_dict. For all other
    wrappers, 'kwargs' are passed positionally after 'env' as standard keyword
    arguments.
    '''

    for spec in wrappers_spec[::-1]:
        name = spec.get("name")
        kwargs = spec.get("kwargs", {}) or {}
        module = spec.get("module", None)
        cls = _resolve_wrapper(name, module)
        env = cls(env, **kwargs)
    return env


job = get_qm_job()
physical_qubits = (0,)
target_name = "rx"
target = GateTarget(gate=target_name, physical_qubits=physical_qubits)


# Instantiate reward from embedded spec. The spec contains the class name,
# an optional module path, and the constructor kwargs captured from the original
# environment via reward.reward_args when available.
reward = _instantiate_reward_from_spec(_REWARD_SPEC)

# Action space specification
param_bounds = [(-1.9800000190734863, 2.0)]  # Can be any number of bounds

# Environment execution parameters
seed = 1203  # Master seed to make training reproducible
batch_size = 32  # Number of actions to evaluate per policy evaluation
n_shots = 50  # Minimum number of shots per fiducial evaluation
pauli_sampling = 100  # Number of fiducials to compute for fidelity estimation (DFE only)
n_reps = [1]  # Number of repetitions of the cycle circuit
num_updates = TotalUpdates(50)
backend_config = QMConfig(
    num_updates=num_updates.total_updates,
    input_type="INPUT_STREAM",
    verbosity=2,
    timeout=60
)

execution_config = ExecutionConfig(
    batch_size=batch_size,
    sampling_paulis=pauli_sampling,
    n_shots=n_shots,
    n_reps=n_reps,
    seed=seed,
)


def create_action_space(param_bounds):
    param_bounds = np.array(param_bounds, dtype=np.float32)
    lower_bound, upper_bound = param_bounds.T
    return Box(low=lower_bound, high=upper_bound, shape=(len(param_bounds),), dtype=np.float32)


action_space = create_action_space(param_bounds)
q_env_config = QEnvConfig(
    target=target,
    backend_config=backend_config,
    action_space=action_space,
    execution_config=execution_config,
    reward=reward,
    benchmark_config=BenchmarkConfig(0),  # No benchmark for now)
)
q_env = QMEnvironment(q_env_config, job=job)
# Apply wrapper stack (or default if empty)
env = _apply_wrappers(q_env, _WRAPPERS_SPEC)
ppo_config = PPOConfig.from_dict({'WANDB_CONFIG': {'ENABLED': False, 'PROJECT': None, 'ENTITY': None, 'API_KEY': None}, 'RUN_NAME': 'test', 'OPTIMIZER': 'adam', 'NUM_UPDATES': 1, 'N_EPOCHS': 8, 'MINIBATCH_SIZE': 16, 'LEARNING_RATE': 0.0005, 'GAMMA': 0.99, 'GAE_LAMBDA': 0.95, 'ENTROPY_COEF': 0.05, 'VALUE_LOSS_COEF': 0.5, 'GRADIENT_CLIP': 0.5, 'CLIP_VALUE_LOSS': True, 'CLIP_VALUE_COEF': 0.2, 'CLIP_RATIO': 0.2, 'INPUT_ACTIVATION_FUNCTION': 'identity', 'HIDDEN_lAYERS': [64, 64], 'HIDDEN_ACTIVATION_FUNCTIONS': ['tanh', 'tanh'], 'OUTPUT_ACTIVATION_MEAN': 'tanh', 'OUTPUT_ACTIVATION_STD': 'sigmoid', 'INCLUDE_CRITIC': True, 'NORMALIZE_ADVANTAGE': True, 'TRAINING_CONFIG': {'TOTAL_UPDATES': 50, 'TARGET_FIDELITIES': [0.999, 0.9999, 0.99999], 'LOOKBACK_WINDOW': 10, 'ANNEAL_LEARNING_RATE': False, 'STD_ACTIONS_EPS': '1e-2'}, 'TRAIN_FUNCTION_SETTINGS': {'PLOT_REAL_TIME': False, 'PRINT_DEBUG': False, 'NUM_PRINTS': 10, 'HPO_MODE': False, 'CLEAR_HISTORY': False, 'SAVE_DATA': False}})

ppo_agent = CustomQMPPO(ppo_config, env)

ppo_training = TrainingConfig(num_updates)
ppo_settings = TrainFunctionSettings(plot_real_time=True, print_debug=True)

results = ppo_agent.train(ppo_training, ppo_settings)
results['hardware_runtime'] = q_env.hardware_runtime

print(json.dumps(results))
