# AUTO-GENERATED FILE - DO NOT EDIT MANUALLY
# This file was automatically generated by main.py

import json
from rl_qoc.agent import PPOConfig, TrainingConfig, TrainFunctionSettings, TotalUpdates
from rl_qoc.qua import QMEnvironment, CustomQMPPO, QMConfig
from iqcc_cloud_client.runtime import get_qm_job
from rl_qoc import (
    RescaleAndClipAction,
    GateTarget,
    StateTarget,
    ChannelReward,
    StateReward,
    CAFEReward,
    ExecutionConfig,
    QEnvConfig,
    BenchmarkConfig,
)
import numpy as np
from gymnasium.spaces import Box

job = get_qm_job()

physical_qubits = (0,)
target_state = np.array([[0.+0.j, 0.+0.j],
 [0.+0.j, 1.+0.j]], dtype=complex)
target = StateTarget(state=target_state, physical_qubits=physical_qubits)

reward = StateReward()  # Use the class name to instantiate the reward

# Action space specification
param_bounds = [(-1.98, 2.0)]  # Can be any number of bounds

# Environment execution parameters
seed = 1203  # Master seed to make training reproducible
batch_size = 32  # Number of actions to evaluate per policy evaluation
n_shots = 100  # Minimum number of shots per fiducial evaluation
pauli_sampling = 100  # Number of fiducials to compute for fidelity estimation (DFE only)
n_reps = 1  # Number of repetitions of the cycle circuit
num_updates = TotalUpdates(5)
input_type = "INPUT_STREAM"
backend_config = QMConfig(
    num_updates=num_updates.total_updates,
    input_type=input_type,
    verbosity=2,
)

execution_config = ExecutionConfig(
    batch_size=batch_size,
    sampling_paulis=pauli_sampling,
    n_shots=n_shots,
    n_reps=n_reps,
    seed=seed,
)


def create_action_space(param_bounds):
    param_bounds = np.array(param_bounds, dtype=np.float32)
    lower_bound, upper_bound = param_bounds.T
    return Box(low=lower_bound, high=upper_bound, shape=(len(param_bounds),), dtype=np.float32)


action_space = create_action_space(param_bounds)
q_env_config = QEnvConfig(
    target=target,
    backend_config=backend_config,
    action_space=action_space,
    execution_config=execution_config,
    reward=reward,
    benchmark_config=BenchmarkConfig(0),  # No benchmark for now)
)
q_env = QMEnvironment(q_env_config, job=job)
rescaled_env = RescaleAndClipAction(q_env, -1.0, 1.0)
ppo_config = PPOConfig.from_dict({'WANDB_CONFIG': {'ENABLED': False, 'PROJECT': None, 'ENTITY': None, 'API_KEY': None}, 'RUN_NAME': 'test', 'OPTIMIZER': 'adam', 'NUM_UPDATES': 1, 'N_EPOCHS': 8, 'MINIBATCH_SIZE': 16, 'LEARNING_RATE': 0.0005, 'GAMMA': 0.99, 'GAE_LAMBDA': 0.95, 'ENTROPY_COEF': 0.05, 'VALUE_LOSS_COEF': 0.5, 'GRADIENT_CLIP': 0.5, 'CLIP_VALUE_LOSS': True, 'CLIP_VALUE_COEF': 0.2, 'CLIP_RATIO': 0.2, 'INPUT_ACTIVATION_FUNCTION': 'identity', 'HIDDEN_lAYERS': [64, 64], 'HIDDEN_ACTIVATION_FUNCTIONS': ['tanh', 'tanh'], 'OUTPUT_ACTIVATION_MEAN': 'tanh', 'OUTPUT_ACTIVATION_STD': 'sigmoid', 'INCLUDE_CRITIC': True, 'NORMALIZE_ADVANTAGE': True})

ppo_agent = CustomQMPPO(ppo_config, rescaled_env)

ppo_training = TrainingConfig(num_updates)
ppo_settings = TrainFunctionSettings(plot_real_time=True, print_debug=True)

results = ppo_agent.train(ppo_training, ppo_settings)

print(json.dumps(results))
