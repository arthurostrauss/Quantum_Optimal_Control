RUN_NAME: "test" # Name of the run
OPTIMIZER: "adam" # Optimizer for the policy network
NUM_UPDATES: 1 # Number of policy updates
N_EPOCHS: 8 # Number of epochs for each policy update
MINIBATCH_SIZE: 16 # Number of samples per mini-batch
LR: 0.0005 # Learning rate
GAMMA: 0.99 # Discount factor
GAE_LAMBDA: 0.95 # Lambda coefficient for Generalized Advantage Estimation
ENT_COEF: 0.01 # Entropy coefficient
V_COEF: 0.5 # Value (critic) function coefficient
GRADIENT_CLIP: 0.5 # Gradient clipping
CLIP_VALUE_LOSS: True # Whether to clip value loss
CLIP_VALUE_COEF: 0.2 # Clipping coefficient for value loss
CLIP_RATIO: 0.2 # Clipping ratio for PPO
N_UNITS: [ 64, 64 ] # Number of units in hidden layers
ACTIVATION: "tanh" # Activation functions
INCLUDE_CRITIC: True # Whether to include critic network within ActorCritic model
NORMALIZE_ADVANTAGE: True # Whether to normalize advantage
CHKPT_DIR: "checkpoints" # Directory to save checkpoints
